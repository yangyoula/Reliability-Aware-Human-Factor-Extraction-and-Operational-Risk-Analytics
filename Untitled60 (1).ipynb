{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "htb8JmFjB4SI",
        "outputId": "1a74cccf-96c5-4445-db9d-9e2790754fcf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-576436886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ONE CELL (PAPER-READY, NO-TRANSFORMERS, WITH UPLOAD):\n",
        "# Uses your uploaded files:\n",
        "#   - asrs_narratives_clean(.csv/.xlsx)\n",
        "#   - asrs_paper_dataset(.csv/.xlsx)          [full]\n",
        "#   - asrs_paper_dataset_sample(.csv/.xlsx)   [sample]\n",
        "#\n",
        "# Runs: Rule baseline + TF-IDF(LogReg) baseline\n",
        "# Outputs: metrics + figures + debug CSV + zip + auto-download\n",
        "# ============================================================\n",
        "\n",
        "import os, re, json, random, zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Install deps (safe, minimal) ----------\n",
        "import sys, subprocess\n",
        "def pip_install(pkgs):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\"] + pkgs, check=False)\n",
        "\n",
        "pip_install([\n",
        "    \"numpy==1.26.4\",\n",
        "    \"pandas==2.2.2\",\n",
        "    \"scikit-learn==1.4.2\",\n",
        "    \"matplotlib\",\n",
        "    \"tqdm\",\n",
        "    \"openpyxl\",\n",
        "])\n",
        "\n",
        "# Re-import after install\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"ENV OK:\",\n",
        "      \"numpy\", np.__version__,\n",
        "      \"pandas\", pd.__version__)\n",
        "\n",
        "# ---------- Upload ----------\n",
        "from google.colab import files\n",
        "print(\"\\n[UPLOAD] Please upload these 3 files now:\")\n",
        "print(\" - asrs_narratives_clean.csv (or .xlsx)\")\n",
        "print(\" - asrs_paper_dataset.csv (or .xlsx)\")\n",
        "print(\" - asrs_paper_dataset_sample.csv (or .xlsx)\\n\")\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) > 0, \"No files uploaded.\"\n",
        "\n",
        "# ---------- Helper: load csv/xlsx ----------\n",
        "def load_table(path):\n",
        "    if path.lower().endswith(\".csv\"):\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    if path.lower().endswith(\".xlsx\") or path.lower().endswith(\".xls\"):\n",
        "        return pd.read_excel(path)\n",
        "    raise ValueError(f\"Unsupported file type: {path}\")\n",
        "\n",
        "# ---------- Find files by fuzzy name ----------\n",
        "def pick_file(keys, contains_any):\n",
        "    keys_l = [k.lower() for k in keys]\n",
        "    for i, k in enumerate(keys_l):\n",
        "        if any(s in k for s in contains_any):\n",
        "            return list(keys)[i]\n",
        "    return None\n",
        "\n",
        "keys = list(uploaded.keys())\n",
        "\n",
        "f_narr = pick_file(keys, [\"narratives_clean\", \"narrative_clean\"])\n",
        "f_full = pick_file(keys, [\"paper_dataset.csv\", \"paper_dataset.xlsx\", \"asrs_paper_dataset\"])\n",
        "f_samp = pick_file(keys, [\"paper_dataset_sample\", \"dataset_sample\", \"sample\"])\n",
        "\n",
        "# fallback: if user uploaded exact names\n",
        "if f_samp is None:\n",
        "    for k in keys:\n",
        "        if \"sample\" in k.lower():\n",
        "            f_samp = k; break\n",
        "\n",
        "assert f_samp is not None or f_full is not None, \"I can't find sample/full dataset file among uploads.\"\n",
        "\n",
        "print(\"\\nDetected files:\")\n",
        "print(\" - narratives:\", f_narr)\n",
        "print(\" - full:\", f_full)\n",
        "print(\" - sample:\", f_samp)\n",
        "\n",
        "# ---------- Choose dataset to run (sample first for speed) ----------\n",
        "USE_FULL = False   # <-- set True if you want to run full dataset (will take longer)\n",
        "\n",
        "DATA_FILE = f_full if (USE_FULL and f_full is not None) else f_samp\n",
        "assert DATA_FILE is not None, \"Requested full dataset but not uploaded.\"\n",
        "\n",
        "df = load_table(DATA_FILE)\n",
        "print(f\"\\nUsing dataset: {DATA_FILE} | shape={df.shape}\")\n",
        "\n",
        "# ---------- Validate required columns ----------\n",
        "need_cols = {\"incident_id\", \"narrative\", \"labels\"}\n",
        "missing = need_cols - set([c.lower() for c in df.columns])\n",
        "if missing:\n",
        "    # try case-insensitive mapping\n",
        "    colmap = {c.lower(): c for c in df.columns}\n",
        "    for req in list(need_cols):\n",
        "        if req in colmap:\n",
        "            df.rename(columns={colmap[req]: req}, inplace=True)\n",
        "    missing2 = need_cols - set(df.columns)\n",
        "    assert not missing2, f\"Dataset missing columns: {missing2}. Columns found: {list(df.columns)}\"\n",
        "else:\n",
        "    # normalize to exact names\n",
        "    colmap = {c.lower(): c for c in df.columns}\n",
        "    df.rename(columns={colmap[\"incident_id\"]: \"incident_id\",\n",
        "                       colmap[\"narrative\"]: \"narrative\",\n",
        "                       colmap[\"labels\"]: \"labels\"}, inplace=True)\n",
        "\n",
        "df[\"narrative\"] = df[\"narrative\"].astype(str)\n",
        "\n",
        "# ---------- Config ----------\n",
        "TOPK = 50\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ---------- Parse & clean labels ----------\n",
        "def parse_labels(s):\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return []\n",
        "    return [x.strip() for x in s.split(\";\") if x.strip()]\n",
        "\n",
        "def clean_label(l):\n",
        "    l = str(l)\n",
        "    l = l.replace(\"\\xa0\", \" \").replace(\"&nbsp;\", \" \").replace(\"&nbsp\", \" \")\n",
        "    l = re.sub(r\"\\s+\", \" \", l).strip()\n",
        "    if l == \"\" or l.lower() in {\"none\", \"nan\", \"null\"}:\n",
        "        return \"\"\n",
        "    return l\n",
        "\n",
        "df[\"gold\"] = df[\"labels\"].apply(parse_labels)\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [clean_label(x) for x in labs])\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [x for x in labs if x])\n",
        "\n",
        "# Build TopK after cleaning\n",
        "all_gold = [lab for labs in df[\"gold\"] for lab in labs]\n",
        "freq = pd.Series(all_gold).value_counts()\n",
        "label_space = freq.head(TOPK).index.tolist()\n",
        "label_set = set(label_space)\n",
        "\n",
        "df[\"gold_topk\"] = df[\"gold\"].apply(lambda labs: [l for l in labs if l in label_set])\n",
        "df_eval = df[df[\"gold_topk\"].map(len) > 0].copy()\n",
        "\n",
        "print(f\"\\nEval rows after TopK filter: {len(df_eval)} / {len(df)} | TOPK={TOPK}\")\n",
        "print(\"Top 10 labels:\", label_space[:10])\n",
        "\n",
        "# ---------- Metrics ----------\n",
        "mlb = MultiLabelBinarizer(classes=label_space)\n",
        "Y_gold = df_eval[\"gold_topk\"].tolist()\n",
        "\n",
        "def compute_metrics(true_list, pred_list):\n",
        "    yt = mlb.fit_transform(true_list)\n",
        "    yp = mlb.transform(pred_list)\n",
        "    return {\n",
        "        \"micro_f1\": float(f1_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", zero_division=0)),\n",
        "        \"micro_precision\": float(precision_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"micro_recall\": float(recall_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "\n",
        "# ---------- Baseline 1: Rule substring ----------\n",
        "def normalize(s):\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).lower()).strip()\n",
        "\n",
        "label_value = {l: normalize(l.split(\":\", 1)[-1]) for l in label_space}\n",
        "\n",
        "def rule_pred(text):\n",
        "    t = normalize(text)\n",
        "    out = []\n",
        "    for lab in label_space:\n",
        "        key = label_value[lab]\n",
        "        if len(key) < 4:\n",
        "            continue\n",
        "        if key in t:\n",
        "            out.append(lab)\n",
        "    # dedup & sort\n",
        "    out = list(dict.fromkeys(out))\n",
        "    return sorted(out)\n",
        "\n",
        "df_eval[\"pred_rule\"] = df_eval[\"narrative\"].apply(rule_pred)\n",
        "m_rule = compute_metrics(Y_gold, df_eval[\"pred_rule\"].tolist())\n",
        "rows.append({\"method\": \"Rule baseline (substring)\", **m_rule})\n",
        "\n",
        "# ---------- Baseline 2: TF-IDF + OvR Logistic Regression ----------\n",
        "X = df_eval[\"narrative\"].values\n",
        "Ybin = mlb.fit_transform(Y_gold)\n",
        "\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, Ybin, np.arange(len(df_eval)), test_size=TEST_SIZE, random_state=SEED\n",
        ")\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=80000)\n",
        "Xtr = vec.fit_transform(X_train)\n",
        "Xte = vec.transform(X_test)\n",
        "\n",
        "probs = np.zeros((Xte.shape[0], len(label_space)), dtype=float)\n",
        "\n",
        "print(\"\\nTraining TF-IDF + LogReg (OvR over TopK labels)...\")\n",
        "for j, lab in enumerate(label_space):\n",
        "    yj = y_train[:, j]\n",
        "    if yj.sum() < 5:\n",
        "        continue\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "    clf.fit(Xtr, yj)\n",
        "    probs[:, j] = clf.predict_proba(Xte)[:, 1]\n",
        "\n",
        "y_pred = (probs >= 0.5).astype(int)\n",
        "pred_tfidf = [[label_space[j] for j in np.where(row == 1)[0]] for row in y_pred]\n",
        "\n",
        "gold_test = [Y_gold[i] for i in idx_test]\n",
        "m_tfidf = compute_metrics(gold_test, pred_tfidf)\n",
        "rows.append({\"method\": \"TF-IDF + LogReg (test split)\", **m_tfidf})\n",
        "\n",
        "# ---------- Save metrics ----------\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "print(metrics_df)\n",
        "metrics_df.to_csv(\"paper_results_metrics.csv\", index=False)\n",
        "\n",
        "# ---------- Figures (Top-20 support + TF-IDF recall on test split) ----------\n",
        "yt = mlb.fit_transform(gold_test)\n",
        "yp = mlb.transform(pred_tfidf)\n",
        "\n",
        "support = np.asarray(yt.sum(axis=0)).ravel()\n",
        "tp = np.asarray((yt.multiply(yp)).sum(axis=0)).ravel()\n",
        "rec = np.divide(tp, support, out=np.zeros_like(tp, dtype=float), where=support > 0)\n",
        "\n",
        "top_idx = np.argsort(-support)[:20]\n",
        "top_labels = [label_space[i] for i in top_idx]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), support[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Support (count in gold, test split)\")\n",
        "plt.title(\"Top-20 Label Support (ASRS TopK)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_support.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), rec[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Recall (TF-IDF)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"TF-IDF Recall on Top-20 Labels (test split)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_tfidf_recall.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ---------- Debug predictions (paper error analysis) ----------\n",
        "debug = df_eval[[\"incident_id\",\"narrative\",\"gold_topk\"]].copy()\n",
        "debug[\"gold_topk\"] = debug[\"gold_topk\"].apply(lambda x: \"; \".join(x))\n",
        "\n",
        "# add rule for all rows\n",
        "debug[\"pred_rule\"] = df_eval[\"pred_rule\"].apply(lambda x: \"; \".join(x))\n",
        "\n",
        "# add TF-IDF predictions only for test split rows; others blank\n",
        "pred_tfidf_full = [\"\"] * len(df_eval)\n",
        "for local_i, global_i in enumerate(idx_test):\n",
        "    pred_tfidf_full[global_i] = \"; \".join(pred_tfidf[local_i])\n",
        "debug[\"pred_tfidf_testonly\"] = pred_tfidf_full\n",
        "\n",
        "debug.to_csv(\"paper_predictions_debug.csv\", index=False)\n",
        "\n",
        "# ---------- Zip all artifacts ----------\n",
        "artifacts = [\n",
        "    \"paper_results_metrics.csv\",\n",
        "    \"paper_predictions_debug.csv\",\n",
        "    \"figure_top_labels_support.png\",\n",
        "    \"figure_top_labels_tfidf_recall.png\",\n",
        "]\n",
        "zip_name = \"paper_artifacts.zip\"\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in artifacts:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        print(\" -\", f)\n",
        "\n",
        "# ---------- Auto-download ----------\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)\n",
        "\n",
        "print(\"\\nALL DONE.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ONE CELL (SELF-HEAL + UPLOAD + PAPER RESULTS + DOWNLOAD)\n",
        "# - Fixes broken numpy (\"numpy.char\"/\"numpy.strings\" missing) by hard reset\n",
        "# - After reconnect, re-run the SAME cell to execute experiment\n",
        "# - No transformers (robust): Rule baseline + TF-IDF/LogReg baseline\n",
        "# - Outputs + auto-download: metrics, figures, debug csv, zip\n",
        "# ============================================================\n",
        "\n",
        "import os, sys, subprocess, textwrap, zipfile, re, random, signal, time\n",
        "\n",
        "PHASE_FLAG = \"/content/.PHASE2_OK\"\n",
        "\n",
        "def sh(cmd):\n",
        "    return subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True).stdout\n",
        "\n",
        "# -------------------------\n",
        "# PHASE 1: Fix environment + hard restart\n",
        "# -------------------------\n",
        "if not os.path.exists(PHASE_FLAG):\n",
        "    print(\"PHASE 1/2: Repairing broken numpy environment, then restarting runtime...\\n\")\n",
        "\n",
        "    # Try to remove broken numpy dirs (common when mixed installs happened)\n",
        "    print(\"Removing suspicious numpy folders...\")\n",
        "    print(sh(\"python - <<'PY'\\n\"\n",
        "             \"import site, glob, os\\n\"\n",
        "             \"paths=[]\\n\"\n",
        "             \"for p in site.getsitepackages():\\n\"\n",
        "             \"  paths += glob.glob(os.path.join(p,'numpy*'))\\n\"\n",
        "             \"for p in paths:\\n\"\n",
        "             \"  print(' -', p)\\n\"\n",
        "             \"PY\"))\n",
        "\n",
        "    # Force uninstall and reinstall clean stack with no cache\n",
        "    print(\"Uninstalling potentially broken packages...\")\n",
        "    print(sh(\"pip -q uninstall -y numpy pandas scikit-learn scipy || true\"))\n",
        "\n",
        "    # Reinstall pinned versions (Colab-safe). scipy included to avoid sklearn issues.\n",
        "    print(\"Installing clean pinned stack...\")\n",
        "    print(sh(\"pip -q install --no-cache-dir numpy==1.26.4 pandas==2.2.2 scipy==1.11.4 scikit-learn==1.4.2 \"\n",
        "             \"matplotlib tqdm openpyxl\"))\n",
        "\n",
        "    # Sanity check numpy core modules\n",
        "    print(\"Sanity-checking numpy modules...\")\n",
        "    out = sh(\"python - <<'PY'\\n\"\n",
        "             \"import numpy as np\\n\"\n",
        "             \"import numpy.char\\n\"\n",
        "             \"print('numpy OK:', np.__version__)\\n\"\n",
        "             \"print('numpy file:', np.__file__)\\n\"\n",
        "             \"PY\")\n",
        "    print(out)\n",
        "\n",
        "    if \"numpy OK\" not in out:\n",
        "        print(\"\\nStill broken. Forcing runtime kill now; reconnect and run this cell again.\\n\")\n",
        "    else:\n",
        "        # Mark phase 2 ready\n",
        "        open(PHASE_FLAG, \"w\").write(\"ok\")\n",
        "        print(\"\\nEnvironment fixed. Restarting runtime now (required)...\\n\")\n",
        "\n",
        "    # Hard kill kernel to force clean reload of site-packages\n",
        "    time.sleep(1)\n",
        "    os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "# -------------------------\n",
        "# PHASE 2: Upload + run experiment + download artifacts\n",
        "# -------------------------\n",
        "print(\"PHASE 2/2: Running paper-ready baselines...\\n\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"ENV OK:\",\n",
        "      \"numpy\", np.__version__,\n",
        "      \"pandas\", pd.__version__)\n",
        "\n",
        "# ---------- Upload ----------\n",
        "from google.colab import files\n",
        "print(\"\\n[UPLOAD] Upload these files (csv OR xlsx):\")\n",
        "print(\" - asrs_narratives_clean\")\n",
        "print(\" - asrs_paper_dataset\")\n",
        "print(\" - asrs_paper_dataset_sample\\n\")\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) > 0, \"No files uploaded.\"\n",
        "\n",
        "keys = list(uploaded.keys())\n",
        "\n",
        "def load_table(path):\n",
        "    if path.lower().endswith(\".csv\"):\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    if path.lower().endswith(\".xlsx\") or path.lower().endswith(\".xls\"):\n",
        "        return pd.read_excel(path)\n",
        "    raise ValueError(f\"Unsupported file type: {path}\")\n",
        "\n",
        "def pick(keys, patterns):\n",
        "    kl = [k.lower() for k in keys]\n",
        "    for i, k in enumerate(kl):\n",
        "        if any(p in k for p in patterns):\n",
        "            return keys[i]\n",
        "    return None\n",
        "\n",
        "f_samp = pick(keys, [\"paper_dataset_sample\", \"dataset_sample\", \"sample\"])\n",
        "f_full = pick(keys, [\"paper_dataset.csv\", \"paper_dataset.xlsx\", \"asrs_paper_dataset\"])\n",
        "f_narr = pick(keys, [\"narratives_clean\", \"narrative_clean\"])\n",
        "\n",
        "assert f_samp is not None or f_full is not None, \"Cannot find sample/full dataset in uploads.\"\n",
        "\n",
        "print(\"\\nDetected:\")\n",
        "print(\" - narratives:\", f_narr)\n",
        "print(\" - full:\", f_full)\n",
        "print(\" - sample:\", f_samp)\n",
        "\n",
        "# Choose dataset: sample first for speed\n",
        "USE_FULL = False   # set True if you want full\n",
        "DATA_FILE = f_full if (USE_FULL and f_full is not None) else f_samp\n",
        "assert DATA_FILE is not None, \"Requested full but it wasn't uploaded.\"\n",
        "\n",
        "df = load_table(DATA_FILE)\n",
        "print(f\"\\nUsing dataset: {DATA_FILE} | shape={df.shape}\")\n",
        "\n",
        "# ---------- Normalize column names ----------\n",
        "lower = {c.lower(): c for c in df.columns}\n",
        "need = [\"incident_id\", \"narrative\", \"labels\"]\n",
        "for n in need:\n",
        "    assert n in lower, f\"Missing column '{n}' in dataset. Found columns: {list(df.columns)}\"\n",
        "df = df.rename(columns={lower[\"incident_id\"]:\"incident_id\",\n",
        "                        lower[\"narrative\"]:\"narrative\",\n",
        "                        lower[\"labels\"]:\"labels\"})\n",
        "df[\"narrative\"] = df[\"narrative\"].astype(str)\n",
        "\n",
        "# ---------- Config ----------\n",
        "TOPK = 50\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ---------- Labels parse & clean ----------\n",
        "def parse_labels(s):\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return []\n",
        "    return [x.strip() for x in s.split(\";\") if x.strip()]\n",
        "\n",
        "def clean_label(l):\n",
        "    l = str(l)\n",
        "    l = l.replace(\"\\xa0\", \" \").replace(\"&nbsp;\", \" \").replace(\"&nbsp\", \" \")\n",
        "    l = re.sub(r\"\\s+\", \" \", l).strip()\n",
        "    if l == \"\" or l.lower() in {\"none\", \"nan\", \"null\"}:\n",
        "        return \"\"\n",
        "    return l\n",
        "\n",
        "df[\"gold\"] = df[\"labels\"].apply(parse_labels)\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [clean_label(x) for x in labs])\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [x for x in labs if x])\n",
        "\n",
        "all_gold = [lab for labs in df[\"gold\"] for lab in labs]\n",
        "freq = pd.Series(all_gold).value_counts()\n",
        "label_space = freq.head(TOPK).index.tolist()\n",
        "label_set = set(label_space)\n",
        "\n",
        "df[\"gold_topk\"] = df[\"gold\"].apply(lambda labs: [l for l in labs if l in label_set])\n",
        "df_eval = df[df[\"gold_topk\"].map(len) > 0].copy()\n",
        "\n",
        "print(f\"\\nEval rows after TopK filter: {len(df_eval)} / {len(df)} | TOPK={TOPK}\")\n",
        "print(\"Top 10 labels:\", label_space[:10])\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=label_space)\n",
        "Y_gold_all = df_eval[\"gold_topk\"].tolist()\n",
        "\n",
        "def compute_metrics(true_list, pred_list):\n",
        "    yt = mlb.fit_transform(true_list)\n",
        "    yp = mlb.transform(pred_list)\n",
        "    return {\n",
        "        \"micro_f1\": float(f1_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", zero_division=0)),\n",
        "        \"micro_precision\": float(precision_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"micro_recall\": float(recall_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "\n",
        "# ---------- Baseline 1: Rule substring ----------\n",
        "def normalize(s):\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).lower()).strip()\n",
        "\n",
        "label_value = {l: normalize(l.split(\":\", 1)[-1]) for l in label_space}\n",
        "\n",
        "def rule_pred(text):\n",
        "    t = normalize(text)\n",
        "    out = []\n",
        "    for lab in label_space:\n",
        "        key = label_value[lab]\n",
        "        if len(key) < 4:\n",
        "            continue\n",
        "        if key in t:\n",
        "            out.append(lab)\n",
        "    out = list(dict.fromkeys(out))\n",
        "    return sorted(out)\n",
        "\n",
        "df_eval[\"pred_rule\"] = df_eval[\"narrative\"].apply(rule_pred)\n",
        "m_rule = compute_metrics(Y_gold_all, df_eval[\"pred_rule\"].tolist())\n",
        "rows.append({\"method\": \"Rule baseline (substring)\", **m_rule})\n",
        "\n",
        "# ---------- Baseline 2: TF-IDF + OvR Logistic Regression ----------\n",
        "X = df_eval[\"narrative\"].values\n",
        "Ybin = mlb.fit_transform(Y_gold_all)\n",
        "\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, Ybin, np.arange(len(df_eval)), test_size=TEST_SIZE, random_state=SEED\n",
        ")\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=80000)\n",
        "Xtr = vec.fit_transform(X_train)\n",
        "Xte = vec.transform(X_test)\n",
        "\n",
        "probs = np.zeros((Xte.shape[0], len(label_space)), dtype=float)\n",
        "\n",
        "print(\"\\nTraining TF-IDF + LogReg (OvR over TopK labels)...\")\n",
        "for j, lab in enumerate(label_space):\n",
        "    yj = y_train[:, j]\n",
        "    if yj.sum() < 5:\n",
        "        continue\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "    clf.fit(Xtr, yj)\n",
        "    probs[:, j] = clf.predict_proba(Xte)[:, 1]\n",
        "\n",
        "y_pred = (probs >= 0.5).astype(int)\n",
        "pred_tfidf = [[label_space[j] for j in np.where(row == 1)[0]] for row in y_pred]\n",
        "\n",
        "gold_test = [Y_gold_all[i] for i in idx_test]\n",
        "m_tfidf = compute_metrics(gold_test, pred_tfidf)\n",
        "rows.append({\"method\": \"TF-IDF + LogReg (test split)\", **m_tfidf})\n",
        "\n",
        "# ---------- Save metrics ----------\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "print(metrics_df)\n",
        "metrics_df.to_csv(\"paper_results_metrics.csv\", index=False)\n",
        "\n",
        "# ---------- Figures ----------\n",
        "yt = mlb.fit_transform(gold_test)\n",
        "yp = mlb.transform(pred_tfidf)\n",
        "\n",
        "support = np.asarray(yt.sum(axis=0)).ravel()\n",
        "tp = np.asarray((yt.multiply(yp)).sum(axis=0)).ravel()\n",
        "rec = np.divide(tp, support, out=np.zeros_like(tp, dtype=float), where=support > 0)\n",
        "\n",
        "top_idx = np.argsort(-support)[:20]\n",
        "top_labels = [label_space[i] for i in top_idx]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), support[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Support (count in gold, test split)\")\n",
        "plt.title(\"Top-20 Label Support (ASRS TopK)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_support.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), rec[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Recall (TF-IDF)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"TF-IDF Recall on Top-20 Labels (test split)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_tfidf_recall.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ---------- Debug predictions ----------\n",
        "debug = df_eval[[\"incident_id\",\"narrative\",\"gold_topk\"]].copy()\n",
        "debug[\"gold_topk\"] = debug[\"gold_topk\"].apply(lambda x: \"; \".join(x))\n",
        "debug[\"pred_rule\"] = df_eval[\"pred_rule\"].apply(lambda x: \"; \".join(x))\n",
        "\n",
        "pred_tfidf_full = [\"\"] * len(df_eval)\n",
        "for local_i, global_i in enumerate(idx_test):\n",
        "    pred_tfidf_full[global_i] = \"; \".join(pred_tfidf[local_i])\n",
        "debug[\"pred_tfidf_testonly\"] = pred_tfidf_full\n",
        "\n",
        "debug.to_csv(\"paper_predictions_debug.csv\", index=False)\n",
        "\n",
        "# ---------- Zip ----------\n",
        "artifacts = [\n",
        "    \"paper_results_metrics.csv\",\n",
        "    \"paper_predictions_debug.csv\",\n",
        "    \"figure_top_labels_support.png\",\n",
        "    \"figure_top_labels_tfidf_recall.png\",\n",
        "]\n",
        "zip_name = \"paper_artifacts.zip\"\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in artifacts:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        print(\" -\", f)\n",
        "\n",
        "# ---------- Auto-download ----------\n",
        "from google.colab import files\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)\n",
        "\n",
        "print(\"\\nALL DONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXPEumdIRH8G",
        "outputId": "faf983f4-fc5e-4e19-da09-cb36c0c0e432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PHASE 1/2: Repairing broken numpy environment, then restarting runtime...\n",
            "\n",
            "Removing suspicious numpy folders...\n",
            " - /usr/local/lib/python3.12/dist-packages/numpy\n",
            " - /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info\n",
            " - /usr/local/lib/python3.12/dist-packages/numpy.libs\n",
            "\n",
            "Uninstalling potentially broken packages...\n",
            "\n",
            "Installing clean pinned stack...\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 67.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.4/60.4 kB 254.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 244.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 221.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.8/35.8 MB 144.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 146.7 MB/s eta 0:00:00\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "giddy 2.3.8 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.11.4 which is incompatible.\n",
            "libpysal 4.14.1 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "spopt 0.7.0 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "mapclassify 2.10.0 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "inequality 1.1.2 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "esda 2.8.1 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "\n",
            "Sanity-checking numpy modules...\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 2, in <module>\n",
            "ModuleNotFoundError: No module named 'numpy.char'\n",
            "\n",
            "\n",
            "Still broken. Forcing runtime kill now; reconnect and run this cell again.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ONE CELL (CLEAN RESET + UPLOAD + PAPER BASELINES + DOWNLOAD)\n",
        "# Purpose:\n",
        "#   - Fix broken numpy installs in Colab (numpy.char / numpy.strings missing)\n",
        "#   - After reconnect, upload your files and run paper-ready baselines\n",
        "#\n",
        "# Inputs (upload in Phase 2):\n",
        "#   - asrs_narratives_clean(.csv/.xlsx)      [optional]\n",
        "#   - asrs_paper_dataset(.csv/.xlsx)         [optional]\n",
        "#   - asrs_paper_dataset_sample(.csv/.xlsx)  [recommended for speed]\n",
        "#\n",
        "# Outputs:\n",
        "#   - paper_results_metrics.csv\n",
        "#   - paper_predictions_debug.csv\n",
        "#   - figure_top_labels_support.png\n",
        "#   - figure_top_labels_tfidf_recall.png\n",
        "#   - paper_artifacts.zip   (all above packed)\n",
        "#\n",
        "# Notes:\n",
        "#   - This cell intentionally kills the process after fixing env to force restart.\n",
        "#     Colab shows \"session crashed\" -> that's expected.\n",
        "# ============================================================\n",
        "\n",
        "import os, sys, subprocess, time, signal, zipfile, re, random\n",
        "\n",
        "PHASE_FLAG = \"/content/.ASRS_PHASE2_READY\"\n",
        "\n",
        "def run(cmd):\n",
        "    \"\"\"Run a shell command and print output (robust).\"\"\"\n",
        "    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    return p.returncode, p.stdout\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    cmd = \"python -m pip -q install --no-cache-dir \" + \" \".join(pkgs)\n",
        "    return run(cmd)\n",
        "\n",
        "def pip_uninstall(pkgs):\n",
        "    cmd = \"python -m pip -q uninstall -y \" + \" \".join(pkgs) + \" || true\"\n",
        "    return run(cmd)\n",
        "\n",
        "# -------------------------\n",
        "# PHASE 1: Hard repair numpy + restart runtime\n",
        "# -------------------------\n",
        "if not os.path.exists(PHASE_FLAG):\n",
        "    print(\"PHASE 1/2: Repairing Python environment (numpy/pandas/sklearn/scipy)...\\n\")\n",
        "\n",
        "    # 1) Uninstall common conflicting packages\n",
        "    print(\"Uninstalling conflicting packages...\")\n",
        "    _, out = pip_uninstall([\"numpy\",\"pandas\",\"scikit-learn\",\"scipy\",\"transformers\",\"tokenizers\",\"accelerate\"])\n",
        "    print(out)\n",
        "\n",
        "    # 2) Reinstall pinned stable stack (CPU/GPU both OK)\n",
        "    print(\"Installing pinned clean stack (Colab-safe)...\")\n",
        "    code, out = pip_install([\n",
        "        \"numpy==1.26.4\",\n",
        "        \"pandas==2.2.2\",\n",
        "        \"scipy==1.11.4\",\n",
        "        \"scikit-learn==1.4.2\",\n",
        "        \"matplotlib\",\n",
        "        \"tqdm\",\n",
        "        \"openpyxl\"\n",
        "    ])\n",
        "    print(out)\n",
        "\n",
        "    # 3) Sanity check for numpy critical modules\n",
        "    print(\"\\nSanity checking numpy import health...\")\n",
        "    code, out = run(\n",
        "        \"python - <<'PY'\\n\"\n",
        "        \"import numpy as np\\n\"\n",
        "        \"import numpy.char\\n\"\n",
        "        \"print('numpy_version', np.__version__)\\n\"\n",
        "        \"print('numpy_file', np.__file__)\\n\"\n",
        "        \"print('numpy_char_ok')\\n\"\n",
        "        \"PY\"\n",
        "    )\n",
        "    print(out)\n",
        "\n",
        "    if \"numpy_char_ok\" not in out:\n",
        "        print(\"\\n[FAILED] numpy still broken after reinstall.\\n\"\n",
        "              \"Do: Runtime -> Factory reset runtime, then run this cell again.\\n\")\n",
        "        raise RuntimeError(\"numpy still broken\")\n",
        "\n",
        "    # Mark phase 2 ready and force a hard restart\n",
        "    os.makedirs(\"/content\", exist_ok=True)\n",
        "    with open(PHASE_FLAG, \"w\") as f:\n",
        "        f.write(\"ok\")\n",
        "\n",
        "    print(\"\\nEnvironment repaired. Forcing runtime restart now...\\n\"\n",
        "          \"(Colab may say 'session crashed' — that's expected.)\\n\")\n",
        "\n",
        "    time.sleep(1)\n",
        "    os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "# -------------------------\n",
        "# PHASE 2: Upload files + run paper-ready baselines + download\n",
        "# -------------------------\n",
        "print(\"PHASE 2/2: Upload -> Run paper baselines -> Export artifacts\\n\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"ENV OK:\",\n",
        "      \"numpy\", np.__version__,\n",
        "      \"| pandas\", pd.__version__)\n",
        "\n",
        "# -------------------------\n",
        "# Upload\n",
        "# -------------------------\n",
        "from google.colab import files\n",
        "print(\"\\n[UPLOAD] Please upload your files now (csv OR xlsx).\")\n",
        "print(\"Expected names contain keywords like:\")\n",
        "print(\" - narratives_clean\")\n",
        "print(\" - paper_dataset\")\n",
        "print(\" - paper_dataset_sample\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) > 0, \"No files uploaded.\"\n",
        "\n",
        "names = list(uploaded.keys())\n",
        "print(\"\\nUploaded:\")\n",
        "for n in names:\n",
        "    print(\" -\", n)\n",
        "\n",
        "def pick_file(names, must_have_any):\n",
        "    low = [n.lower() for n in names]\n",
        "    for i, n in enumerate(low):\n",
        "        if any(k in n for k in must_have_any):\n",
        "            return names[i]\n",
        "    return None\n",
        "\n",
        "f_sample = pick_file(names, [\"paper_dataset_sample\", \"dataset_sample\", \"sample\"])\n",
        "f_full   = pick_file(names, [\"paper_dataset.csv\", \"paper_dataset.xlsx\", \"asrs_paper_dataset\"])\n",
        "f_narr   = pick_file(names, [\"narratives_clean\", \"narrative_clean\"])\n",
        "\n",
        "print(\"\\nDetected:\")\n",
        "print(\" - narratives_clean:\", f_narr)\n",
        "print(\" - paper_dataset(full):\", f_full)\n",
        "print(\" - paper_dataset_sample:\", f_sample)\n",
        "\n",
        "# Prefer sample for speed, fallback to full if no sample\n",
        "DATA_FILE = f_sample if f_sample is not None else f_full\n",
        "assert DATA_FILE is not None, \"Cannot find sample or full dataset in your uploads.\"\n",
        "\n",
        "def load_table(path):\n",
        "    path_low = path.lower()\n",
        "    if path_low.endswith(\".csv\"):\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    if path_low.endswith(\".xlsx\") or path_low.endswith(\".xls\"):\n",
        "        return pd.read_excel(path)\n",
        "    raise ValueError(f\"Unsupported file type: {path}\")\n",
        "\n",
        "df = load_table(DATA_FILE)\n",
        "print(f\"\\nUsing dataset: {DATA_FILE} | shape={df.shape}\")\n",
        "\n",
        "# -------------------------\n",
        "# Normalize columns\n",
        "# -------------------------\n",
        "cols_map = {c.lower(): c for c in df.columns}\n",
        "\n",
        "for need in [\"incident_id\", \"narrative\", \"labels\"]:\n",
        "    assert need in cols_map, f\"Missing column '{need}'. Found: {list(df.columns)}\"\n",
        "\n",
        "df = df.rename(columns={\n",
        "    cols_map[\"incident_id\"]:\"incident_id\",\n",
        "    cols_map[\"narrative\"]:\"narrative\",\n",
        "    cols_map[\"labels\"]:\"labels\"\n",
        "})\n",
        "\n",
        "df[\"narrative\"] = df[\"narrative\"].astype(str)\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TOPK = 50\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Labels parse + clean\n",
        "# -------------------------\n",
        "def parse_labels(s):\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return []\n",
        "    return [x.strip() for x in s.split(\";\") if x.strip()]\n",
        "\n",
        "def clean_label(l):\n",
        "    l = str(l)\n",
        "    l = l.replace(\"\\xa0\", \" \").replace(\"&nbsp;\", \" \").replace(\"&nbsp\", \" \")\n",
        "    l = re.sub(r\"\\s+\", \" \", l).strip()\n",
        "    if l == \"\" or l.lower() in {\"none\",\"nan\",\"null\"}:\n",
        "        return \"\"\n",
        "    return l\n",
        "\n",
        "df[\"gold\"] = df[\"labels\"].apply(parse_labels)\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [clean_label(x) for x in labs])\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [x for x in labs if x])\n",
        "\n",
        "all_gold = [lab for labs in df[\"gold\"] for lab in labs]\n",
        "freq = pd.Series(all_gold).value_counts()\n",
        "label_space = freq.head(TOPK).index.tolist()\n",
        "label_set = set(label_space)\n",
        "\n",
        "df[\"gold_topk\"] = df[\"gold\"].apply(lambda labs: [l for l in labs if l in label_set])\n",
        "df_eval = df[df[\"gold_topk\"].map(len) > 0].copy()\n",
        "\n",
        "print(f\"\\nTopK={TOPK} | Eval rows={len(df_eval)}/{len(df)}\")\n",
        "print(\"Top 10 labels:\", label_space[:10])\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=label_space)\n",
        "Y_gold_all = df_eval[\"gold_topk\"].tolist()\n",
        "\n",
        "def compute_metrics(true_list, pred_list):\n",
        "    yt = mlb.fit_transform(true_list)\n",
        "    yp = mlb.transform(pred_list)\n",
        "    return {\n",
        "        \"micro_f1\": float(f1_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", zero_division=0)),\n",
        "        \"micro_precision\": float(precision_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"micro_recall\": float(recall_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "\n",
        "# -------------------------\n",
        "# Baseline A: Rule substring match\n",
        "# -------------------------\n",
        "def normalize(s):\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).lower()).strip()\n",
        "\n",
        "label_value = {l: normalize(l.split(\":\", 1)[-1]) for l in label_space}\n",
        "\n",
        "def rule_pred(text):\n",
        "    t = normalize(text)\n",
        "    out = []\n",
        "    for lab in label_space:\n",
        "        key = label_value[lab]\n",
        "        if len(key) < 4:\n",
        "            continue\n",
        "        if key in t:\n",
        "            out.append(lab)\n",
        "    return sorted(list(dict.fromkeys(out)))\n",
        "\n",
        "df_eval[\"pred_rule\"] = df_eval[\"narrative\"].apply(rule_pred)\n",
        "m_rule = compute_metrics(Y_gold_all, df_eval[\"pred_rule\"].tolist())\n",
        "rows.append({\"method\":\"Rule baseline (substring)\", **m_rule})\n",
        "\n",
        "# -------------------------\n",
        "# Baseline B: TF-IDF + OvR Logistic Regression\n",
        "# -------------------------\n",
        "X = df_eval[\"narrative\"].values\n",
        "Ybin = mlb.fit_transform(Y_gold_all)\n",
        "\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, Ybin, np.arange(len(df_eval)),\n",
        "    test_size=TEST_SIZE, random_state=SEED\n",
        ")\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=80000)\n",
        "Xtr = vec.fit_transform(X_train)\n",
        "Xte = vec.transform(X_test)\n",
        "\n",
        "probs = np.zeros((Xte.shape[0], len(label_space)), dtype=float)\n",
        "\n",
        "print(\"\\nTraining TF-IDF + LogisticRegression (OvR) ...\")\n",
        "for j, lab in enumerate(label_space):\n",
        "    yj = y_train[:, j]\n",
        "    if yj.sum() < 5:\n",
        "        continue\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "    clf.fit(Xtr, yj)\n",
        "    probs[:, j] = clf.predict_proba(Xte)[:, 1]\n",
        "\n",
        "y_pred = (probs >= 0.5).astype(int)\n",
        "pred_tfidf = [[label_space[j] for j in np.where(row == 1)[0]] for row in y_pred]\n",
        "\n",
        "gold_test = [Y_gold_all[i] for i in idx_test]\n",
        "m_tfidf = compute_metrics(gold_test, pred_tfidf)\n",
        "rows.append({\"method\":\"TF-IDF + LogReg (test split)\", **m_tfidf})\n",
        "\n",
        "# -------------------------\n",
        "# Save metrics\n",
        "# -------------------------\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "print(metrics_df)\n",
        "metrics_df.to_csv(\"paper_results_metrics.csv\", index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Figures (Top-20 support + recall on test split)\n",
        "# -------------------------\n",
        "yt = mlb.fit_transform(gold_test)\n",
        "yp = mlb.transform(pred_tfidf)\n",
        "\n",
        "support = np.asarray(yt.sum(axis=0)).ravel()\n",
        "tp = np.asarray((yt.multiply(yp)).sum(axis=0)).ravel()\n",
        "rec = np.divide(tp, support, out=np.zeros_like(tp, dtype=float), where=support > 0)\n",
        "\n",
        "top_idx = np.argsort(-support)[:20]\n",
        "top_labels = [label_space[i] for i in top_idx]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), support[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Support (count in gold, test split)\")\n",
        "plt.title(\"Top-20 Label Support (ASRS TopK)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_support.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), rec[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Recall (TF-IDF)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"TF-IDF Recall on Top-20 Labels (test split)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_tfidf_recall.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Debug predictions\n",
        "# -------------------------\n",
        "debug = df_eval[[\"incident_id\",\"narrative\",\"gold_topk\"]].copy()\n",
        "debug[\"gold_topk\"] = debug[\"gold_topk\"].apply(lambda x: \"; \".join(x))\n",
        "debug[\"pred_rule\"] = df_eval[\"pred_rule\"].apply(lambda x: \"; \".join(x))\n",
        "\n",
        "# Only fill TF-IDF predictions for test split indices\n",
        "pred_tfidf_full = [\"\"] * len(df_eval)\n",
        "for local_i, global_i in enumerate(idx_test):\n",
        "    pred_tfidf_full[global_i] = \"; \".join(pred_tfidf[local_i])\n",
        "debug[\"pred_tfidf_testonly\"] = pred_tfidf_full\n",
        "\n",
        "debug.to_csv(\"paper_predictions_debug.csv\", index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Zip + download\n",
        "# -------------------------\n",
        "artifacts = [\n",
        "    \"paper_results_metrics.csv\",\n",
        "    \"paper_predictions_debug.csv\",\n",
        "    \"figure_top_labels_support.png\",\n",
        "    \"figure_top_labels_tfidf_recall.png\",\n",
        "]\n",
        "zip_name = \"paper_artifacts.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in artifacts:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        print(\" -\", f)\n",
        "\n",
        "from google.colab import files\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)\n",
        "\n",
        "print(\"\\nALL DONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdTtnWdGT3HF",
        "outputId": "2c553e7e-19ff-4c48-d290-67ea9f4d84cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PHASE 1/2: Repairing Python environment (numpy/pandas/sklearn/scipy)...\n",
            "\n",
            "Uninstalling conflicting packages...\n",
            "\n",
            "Installing pinned clean stack (Colab-safe)...\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 74.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.4/60.4 kB 127.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 108.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 77.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.8/35.8 MB 43.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 57.5 MB/s eta 0:00:00\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, which is not installed.\n",
            "torchtune 0.6.1 requires tokenizers, which is not installed.\n",
            "peft 0.18.1 requires accelerate>=0.21.0, which is not installed.\n",
            "peft 0.18.1 requires transformers, which is not installed.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "giddy 2.3.8 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.11.4 which is incompatible.\n",
            "libpysal 4.14.1 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "spopt 0.7.0 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "mapclassify 2.10.0 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "inequality 1.1.2 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "esda 2.8.1 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "\n",
            "\n",
            "Sanity checking numpy import health...\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 2, in <module>\n",
            "ModuleNotFoundError: No module named 'numpy.char'\n",
            "\n",
            "\n",
            "[FAILED] numpy still broken after reinstall.\n",
            "Do: Runtime -> Factory reset runtime, then run this cell again.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "numpy still broken",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-724852787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         print(\"\\n[FAILED] numpy still broken after reinstall.\\n\"\n\u001b[1;32m     80\u001b[0m               \"Do: Runtime -> Factory reset runtime, then run this cell again.\\n\")\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numpy still broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Mark phase 2 ready and force a hard restart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: numpy still broken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ONE CELL (MOST STABLE): Fix broken numpy -> Upload -> Paper baselines -> Download\n",
        "# - Fix numpy \"numpy.char\"/\"numpy.strings\" missing by reinstalling a clean stack\n",
        "# - No transformers/torch: avoids GPU/LLM dependency conflicts\n",
        "# Input upload: asrs_paper_dataset_sample.(csv/xlsx)  [preferred]\n",
        "#               asrs_paper_dataset.(csv/xlsx)         [fallback]\n",
        "#               asrs_narratives_clean.(csv/xlsx)      [optional, not required]\n",
        "# Output: metrics + figures + debug CSV + zip + auto-download\n",
        "# ============================================================\n",
        "\n",
        "import os, sys, subprocess, time, signal, zipfile, re, random\n",
        "\n",
        "PHASE_FLAG = \"/content/.ASRS_BASELINE_PHASE2_READY\"\n",
        "\n",
        "def sh(cmd):\n",
        "    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    return p.returncode, p.stdout\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    cmd = \"python -m pip -q install --no-cache-dir \" + \" \".join(pkgs)\n",
        "    return sh(cmd)\n",
        "\n",
        "def pip_uninstall(pkgs):\n",
        "    cmd = \"python -m pip -q uninstall -y \" + \" \".join(pkgs) + \" || true\"\n",
        "    return sh(cmd)\n",
        "\n",
        "# -------------------------\n",
        "# PHASE 1: repair numpy stack and restart (only once)\n",
        "# -------------------------\n",
        "if not os.path.exists(PHASE_FLAG):\n",
        "    print(\"PHASE 1/2: Repairing environment (clean numpy stack)...\\n\")\n",
        "\n",
        "    # Remove possibly broken core libs\n",
        "    print(\"Uninstalling numpy/pandas/scipy/sklearn (ignore errors)...\")\n",
        "    _, out = pip_uninstall([\"numpy\",\"pandas\",\"scipy\",\"scikit-learn\",\"openpyxl\"])\n",
        "    print(out)\n",
        "\n",
        "    print(\"Installing pinned clean stack (for paper baselines)...\")\n",
        "    code, out = pip_install([\n",
        "        \"numpy==1.26.4\",\n",
        "        \"pandas==2.2.2\",\n",
        "        \"scipy==1.11.4\",\n",
        "        \"scikit-learn==1.4.2\",\n",
        "        \"matplotlib\",\n",
        "        \"tqdm\",\n",
        "        \"openpyxl\"\n",
        "    ])\n",
        "    print(out)\n",
        "\n",
        "    print(\"\\nSanity check numpy core modules...\")\n",
        "    code, out = sh(\n",
        "        \"python - <<'PY'\\n\"\n",
        "        \"import numpy as np\\n\"\n",
        "        \"import numpy.char\\n\"\n",
        "        \"print('numpy_version', np.__version__)\\n\"\n",
        "        \"print('numpy_char_ok')\\n\"\n",
        "        \"PY\"\n",
        "    )\n",
        "    print(out)\n",
        "\n",
        "    if \"numpy_char_ok\" not in out:\n",
        "        print(\"\\n[FAILED] numpy still broken.\\n\"\n",
        "              \"Do: Runtime -> Factory reset runtime, then run this cell again.\\n\")\n",
        "        raise RuntimeError(\"numpy still broken\")\n",
        "\n",
        "    with open(PHASE_FLAG, \"w\") as f:\n",
        "        f.write(\"ok\")\n",
        "\n",
        "    print(\"\\nEnvironment fixed. Forcing restart now (Colab may show 'crashed' — normal).\")\n",
        "    time.sleep(1)\n",
        "    os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "# -------------------------\n",
        "# PHASE 2: upload + run paper baselines\n",
        "# -------------------------\n",
        "print(\"PHASE 2/2: Upload files -> Run baselines -> Export artifacts\\n\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"ENV OK:\", \"numpy\", np.__version__, \"| pandas\", pd.__version__)\n",
        "\n",
        "# -------------------------\n",
        "# Upload\n",
        "# -------------------------\n",
        "from google.colab import files\n",
        "print(\"\\n[UPLOAD] Upload your dataset files now (csv OR xlsx).\")\n",
        "print(\"Recommended: asrs_paper_dataset_sample.csv (or .xlsx)\\n\")\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) > 0, \"No files uploaded.\"\n",
        "\n",
        "names = list(uploaded.keys())\n",
        "print(\"\\nUploaded:\")\n",
        "for n in names:\n",
        "    print(\" -\", n)\n",
        "\n",
        "def pick(names, keywords):\n",
        "    low = [n.lower() for n in names]\n",
        "    for i, n in enumerate(low):\n",
        "        if any(k in n for k in keywords):\n",
        "            return names[i]\n",
        "    return None\n",
        "\n",
        "f_sample = pick(names, [\"paper_dataset_sample\", \"dataset_sample\", \"sample\"])\n",
        "f_full   = pick(names, [\"paper_dataset\", \"asrs_paper_dataset\"])\n",
        "DATA_FILE = f_sample if f_sample else f_full\n",
        "assert DATA_FILE is not None, \"Didn't find paper_dataset_sample or paper_dataset in your uploads.\"\n",
        "\n",
        "def load_table(path):\n",
        "    p = path.lower()\n",
        "    if p.endswith(\".csv\"):\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    if p.endswith(\".xlsx\") or p.endswith(\".xls\"):\n",
        "        return pd.read_excel(path)\n",
        "    raise ValueError(f\"Unsupported file: {path}\")\n",
        "\n",
        "df = load_table(DATA_FILE)\n",
        "print(f\"\\nUsing: {DATA_FILE} | shape={df.shape}\")\n",
        "\n",
        "# -------------------------\n",
        "# Normalize required columns\n",
        "# -------------------------\n",
        "cols_map = {c.lower(): c for c in df.columns}\n",
        "need = [\"incident_id\", \"narrative\", \"labels\"]\n",
        "for c in need:\n",
        "    assert c in cols_map, f\"Missing column '{c}'. Found: {list(df.columns)}\"\n",
        "\n",
        "df = df.rename(columns={\n",
        "    cols_map[\"incident_id\"]:\"incident_id\",\n",
        "    cols_map[\"narrative\"]:\"narrative\",\n",
        "    cols_map[\"labels\"]:\"labels\"\n",
        "})\n",
        "df[\"narrative\"] = df[\"narrative\"].astype(str)\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TOPK = 50\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Parse + clean labels\n",
        "# -------------------------\n",
        "def parse_labels(s):\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return []\n",
        "    return [x.strip() for x in s.split(\";\") if x.strip()]\n",
        "\n",
        "def clean_label(l):\n",
        "    l = str(l)\n",
        "    l = l.replace(\"\\xa0\", \" \").replace(\"&nbsp;\", \" \").replace(\"&nbsp\", \" \")\n",
        "    l = re.sub(r\"\\s+\", \" \", l).strip()\n",
        "    if l == \"\" or l.lower() in {\"none\",\"nan\",\"null\"}:\n",
        "        return \"\"\n",
        "    return l\n",
        "\n",
        "df[\"gold\"] = df[\"labels\"].apply(parse_labels)\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [clean_label(x) for x in labs])\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [x for x in labs if x])\n",
        "\n",
        "all_gold = [lab for labs in df[\"gold\"] for lab in labs]\n",
        "freq = pd.Series(all_gold).value_counts()\n",
        "label_space = freq.head(TOPK).index.tolist()\n",
        "label_set = set(label_space)\n",
        "\n",
        "df[\"gold_topk\"] = df[\"gold\"].apply(lambda labs: [l for l in labs if l in label_set])\n",
        "df_eval = df[df[\"gold_topk\"].map(len) > 0].copy()\n",
        "\n",
        "print(f\"\\nTOPK={TOPK} | eval rows={len(df_eval)}/{len(df)}\")\n",
        "print(\"Top 10 labels:\", label_space[:10])\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=label_space)\n",
        "\n",
        "def compute_metrics(true_list, pred_list):\n",
        "    yt = mlb.fit_transform(true_list)\n",
        "    yp = mlb.transform(pred_list)\n",
        "    return {\n",
        "        \"micro_f1\": float(f1_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", zero_division=0)),\n",
        "        \"micro_precision\": float(precision_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"micro_recall\": float(recall_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "Y_gold_all = df_eval[\"gold_topk\"].tolist()\n",
        "\n",
        "# -------------------------\n",
        "# Baseline 1: Rule substring\n",
        "# -------------------------\n",
        "def normalize(s):\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).lower()).strip()\n",
        "\n",
        "label_value = {l: normalize(l.split(\":\", 1)[-1]) for l in label_space}\n",
        "\n",
        "def rule_pred(text):\n",
        "    t = normalize(text)\n",
        "    out = []\n",
        "    for lab in label_space:\n",
        "        key = label_value[lab]\n",
        "        if len(key) < 4:\n",
        "            continue\n",
        "        if key in t:\n",
        "            out.append(lab)\n",
        "    return sorted(list(dict.fromkeys(out)))\n",
        "\n",
        "df_eval[\"pred_rule\"] = df_eval[\"narrative\"].apply(rule_pred)\n",
        "m_rule = compute_metrics(Y_gold_all, df_eval[\"pred_rule\"].tolist())\n",
        "rows.append({\"method\":\"Rule baseline (substring)\", **m_rule})\n",
        "\n",
        "# -------------------------\n",
        "# Baseline 2: TF-IDF + OvR Logistic Regression\n",
        "# -------------------------\n",
        "X = df_eval[\"narrative\"].values\n",
        "Ybin = mlb.fit_transform(Y_gold_all)\n",
        "\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, Ybin, np.arange(len(df_eval)),\n",
        "    test_size=TEST_SIZE, random_state=SEED\n",
        ")\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=80000)\n",
        "Xtr = vec.fit_transform(X_train)\n",
        "Xte = vec.transform(X_test)\n",
        "\n",
        "probs = np.zeros((Xte.shape[0], len(label_space)), dtype=float)\n",
        "\n",
        "print(\"\\nTraining TF-IDF + LogisticRegression (OvR)...\")\n",
        "for j, lab in enumerate(label_space):\n",
        "    yj = y_train[:, j]\n",
        "    if yj.sum() < 5:\n",
        "        continue\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "    clf.fit(Xtr, yj)\n",
        "    probs[:, j] = clf.predict_proba(Xte)[:, 1]\n",
        "\n",
        "y_pred = (probs >= 0.5).astype(int)\n",
        "pred_tfidf = [[label_space[j] for j in np.where(row == 1)[0]] for row in y_pred]\n",
        "\n",
        "gold_test = [Y_gold_all[i] for i in idx_test]\n",
        "m_tfidf = compute_metrics(gold_test, pred_tfidf)\n",
        "rows.append({\"method\":\"TF-IDF + LogReg (test split)\", **m_tfidf})\n",
        "\n",
        "# -------------------------\n",
        "# Save metrics\n",
        "# -------------------------\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "print(metrics_df)\n",
        "metrics_df.to_csv(\"paper_results_metrics.csv\", index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Figures\n",
        "# -------------------------\n",
        "yt = mlb.fit_transform(gold_test)\n",
        "yp = mlb.transform(pred_tfidf)\n",
        "\n",
        "support = np.asarray(yt.sum(axis=0)).ravel()\n",
        "tp = np.asarray((yt.multiply(yp)).sum(axis=0)).ravel()\n",
        "rec = np.divide(tp, support, out=np.zeros_like(tp, dtype=float), where=support > 0)\n",
        "\n",
        "top_idx = np.argsort(-support)[:20]\n",
        "top_labels = [label_space[i] for i in top_idx]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), support[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Support (count in gold, test split)\")\n",
        "plt.title(\"Top-20 Label Support (ASRS TopK)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_support.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), rec[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Recall (TF-IDF)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"TF-IDF Recall on Top-20 Labels (test split)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_tfidf_recall.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Debug CSV\n",
        "# -------------------------\n",
        "debug = df_eval[[\"incident_id\",\"narrative\",\"gold_topk\"]].copy()\n",
        "debug[\"gold_topk\"] = debug[\"gold_topk\"].apply(lambda x: \"; \".join(x))\n",
        "debug[\"pred_rule\"] = df_eval[\"pred_rule\"].apply(lambda x: \"; \".join(x))\n",
        "\n",
        "pred_tfidf_full = [\"\"] * len(df_eval)\n",
        "for local_i, global_i in enumerate(idx_test):\n",
        "    pred_tfidf_full[global_i] = \"; \".join(pred_tfidf[local_i])\n",
        "debug[\"pred_tfidf_testonly\"] = pred_tfidf_full\n",
        "\n",
        "debug.to_csv(\"paper_predictions_debug.csv\", index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Zip + auto download\n",
        "# -------------------------\n",
        "artifacts = [\n",
        "    \"paper_results_metrics.csv\",\n",
        "    \"paper_predictions_debug.csv\",\n",
        "    \"figure_top_labels_support.png\",\n",
        "    \"figure_top_labels_tfidf_recall.png\",\n",
        "]\n",
        "zip_name = \"paper_artifacts.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in artifacts:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        print(\" -\", f)\n",
        "\n",
        "from google.colab import files\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)\n",
        "\n",
        "print(\"\\nALL DONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "36-gjzgXVm9u",
        "outputId": "2dda4042-38f9-43ec-fa9d-c8727988128d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PHASE 1/2: Repairing environment (clean numpy stack)...\n",
            "\n",
            "Uninstalling numpy/pandas/scipy/sklearn (ignore errors)...\n",
            "\n",
            "Installing pinned clean stack (for paper baselines)...\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 5.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.4/60.4 kB 152.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 255.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 211.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.8/35.8 MB 190.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 221.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.9/250.9 kB 267.8 MB/s eta 0:00:00\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, which is not installed.\n",
            "torchtune 0.6.1 requires tokenizers, which is not installed.\n",
            "peft 0.18.1 requires accelerate>=0.21.0, which is not installed.\n",
            "peft 0.18.1 requires transformers, which is not installed.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "giddy 2.3.8 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.11.4 which is incompatible.\n",
            "libpysal 4.14.1 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "spopt 0.7.0 requires scipy>=1.12.0, but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "mapclassify 2.10.0 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "inequality 1.1.2 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "esda 2.8.1 requires scipy>=1.12, but you have scipy 1.11.4 which is incompatible.\n",
            "\n",
            "\n",
            "Sanity check numpy core modules...\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 2, in <module>\n",
            "ModuleNotFoundError: No module named 'numpy.char'\n",
            "\n",
            "\n",
            "[FAILED] numpy still broken.\n",
            "Do: Runtime -> Factory reset runtime, then run this cell again.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "numpy still broken",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1469280260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         print(\"\\n[FAILED] numpy still broken.\\n\"\n\u001b[1;32m     63\u001b[0m               \"Do: Runtime -> Factory reset runtime, then run this cell again.\\n\")\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numpy still broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHASE_FLAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: numpy still broken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 2 (FIXED): UPLOAD -> CLEAN LABELS -> FAST BASELINES -> EXPORT -> DOWNLOAD =====\n",
        "import os, re, zipfile, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "print(\"ENV:\", \"numpy\", np.__version__, \"| pandas\", pd.__version__)\n",
        "\n",
        "# -------------------------\n",
        "# 0) Upload\n",
        "# -------------------------\n",
        "print(\"\\n[UPLOAD] Please upload your files (csv/xlsx):\")\n",
        "print(\" - asrs_paper_dataset_sample.*  (preferred)\")\n",
        "print(\" - asrs_paper_dataset.*         (fallback)\")\n",
        "print(\" - asrs_narratives_clean.*      (optional)\\n\")\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) > 0, \"No files uploaded.\"\n",
        "\n",
        "names = list(uploaded.keys())\n",
        "print(\"\\nUploaded:\")\n",
        "for n in names:\n",
        "    print(\" -\", n)\n",
        "\n",
        "def pick(names, keywords):\n",
        "    low = [n.lower() for n in names]\n",
        "    for i, n in enumerate(low):\n",
        "        if any(k in n for k in keywords):\n",
        "            return names[i]\n",
        "    return None\n",
        "\n",
        "f_sample = pick(names, [\"paper_dataset_sample\", \"dataset_sample\", \"sample\"])\n",
        "f_full   = pick(names, [\"paper_dataset\", \"asrs_paper_dataset\"])\n",
        "DATA_FILE = f_sample if f_sample else f_full\n",
        "assert DATA_FILE is not None, \"Can't find asrs_paper_dataset_sample or asrs_paper_dataset in uploads.\"\n",
        "\n",
        "def load_table(path):\n",
        "    p = path.lower()\n",
        "    if p.endswith(\".csv\"):\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    if p.endswith(\".xlsx\") or p.endswith(\".xls\"):\n",
        "        return pd.read_excel(path)\n",
        "    raise ValueError(f\"Unsupported file type: {path}\")\n",
        "\n",
        "df = load_table(DATA_FILE)\n",
        "print(f\"\\nUsing: {DATA_FILE} | shape={df.shape}\")\n",
        "\n",
        "# -------------------------\n",
        "# 1) Normalize columns\n",
        "# -------------------------\n",
        "cols_map = {c.lower(): c for c in df.columns}\n",
        "for c in [\"incident_id\",\"narrative\",\"labels\"]:\n",
        "    assert c in cols_map, f\"Missing required column '{c}'. Found: {list(df.columns)}\"\n",
        "\n",
        "df = df.rename(columns={\n",
        "    cols_map[\"incident_id\"]:\"incident_id\",\n",
        "    cols_map[\"narrative\"]:\"narrative\",\n",
        "    cols_map[\"labels\"]:\"labels\"\n",
        "})\n",
        "\n",
        "df[\"narrative\"] = df[\"narrative\"].astype(str)\n",
        "\n",
        "# -------------------------\n",
        "# 2) Config\n",
        "# -------------------------\n",
        "TOPK = 50\n",
        "SEED = 42\n",
        "TEST_SIZE = 0.2\n",
        "FAST_ML_BASELINE = True   # True=SGD(快)；False=不跑ML baseline\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Label parsing + CLEAN (关键修复：丢掉 \"xxx: nan\")\n",
        "# -------------------------\n",
        "def parse_labels(s):\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return []\n",
        "    return [x.strip() for x in s.split(\";\") if x.strip()]\n",
        "\n",
        "def clean_label(l):\n",
        "    l = str(l)\n",
        "    l = l.replace(\"\\xa0\", \" \").replace(\"&nbsp;\", \" \").replace(\"&nbsp\", \" \")\n",
        "    l = re.sub(r\"\\s+\", \" \", l).strip()\n",
        "\n",
        "    if l == \"\" or l.lower() in {\"none\",\"nan\",\"null\"}:\n",
        "        return \"\"\n",
        "\n",
        "    # ✅ 关键：如果是 \"Field: Value\"，Value 是 nan/none/空 直接丢掉\n",
        "    if \":\" in l:\n",
        "        k, v = l.split(\":\", 1)\n",
        "        k = k.strip()\n",
        "        v = v.strip()\n",
        "        if v.lower() in {\"nan\", \"none\", \"null\", \"\"}:\n",
        "            return \"\"\n",
        "        return f\"{k}: {v}\"\n",
        "\n",
        "    return l\n",
        "\n",
        "df[\"gold\"] = df[\"labels\"].apply(parse_labels)\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [clean_label(x) for x in labs])\n",
        "df[\"gold\"] = df[\"gold\"].apply(lambda labs: [x for x in labs if x])\n",
        "\n",
        "# ✅ TopK AFTER cleaning\n",
        "all_gold = [lab for labs in df[\"gold\"] for lab in labs]\n",
        "freq = pd.Series(all_gold).value_counts()\n",
        "label_space = freq.head(TOPK).index.tolist()\n",
        "label_set = set(label_space)\n",
        "\n",
        "df[\"gold_topk\"] = df[\"gold\"].apply(lambda labs: [l for l in labs if l in label_set])\n",
        "df_eval = df[df[\"gold_topk\"].map(len) > 0].copy()\n",
        "\n",
        "print(f\"\\nTOPK={TOPK} | eval rows={len(df_eval)}/{len(df)}\")\n",
        "print(\"Top 10 labels:\", label_space[:10])\n",
        "assert all(\": nan\" not in x.lower() for x in label_space), \"TopK still contains ': nan' -> check your input labels format.\"\n",
        "\n",
        "# -------------------------\n",
        "# 4) Metrics helpers\n",
        "# -------------------------\n",
        "mlb = MultiLabelBinarizer(classes=label_space)\n",
        "\n",
        "def compute_metrics(true_list, pred_list):\n",
        "    yt = mlb.fit_transform(true_list)\n",
        "    yp = mlb.transform(pred_list)\n",
        "    return {\n",
        "        \"micro_f1\": float(f1_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", zero_division=0)),\n",
        "        \"micro_precision\": float(precision_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "        \"micro_recall\": float(recall_score(yt, yp, average=\"micro\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "Y_gold_all = df_eval[\"gold_topk\"].tolist()\n",
        "\n",
        "# -------------------------\n",
        "# 5) Baseline 1: rule substring\n",
        "# -------------------------\n",
        "def normalize(s):\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).lower()).strip()\n",
        "\n",
        "label_value = {l: normalize(l.split(\":\", 1)[-1]) for l in label_space}\n",
        "\n",
        "def rule_pred(text):\n",
        "    t = normalize(text)\n",
        "    out = []\n",
        "    for lab in label_space:\n",
        "        key = label_value[lab]\n",
        "        if len(key) < 4:\n",
        "            continue\n",
        "        if key in t:\n",
        "            out.append(lab)\n",
        "    return sorted(list(dict.fromkeys(out)))\n",
        "\n",
        "df_eval[\"pred_rule\"] = df_eval[\"narrative\"].apply(rule_pred)\n",
        "m_rule = compute_metrics(Y_gold_all, df_eval[\"pred_rule\"].tolist())\n",
        "rows.append({\"method\":\"Rule baseline (substring)\", **m_rule})\n",
        "\n",
        "# -------------------------\n",
        "# 6) Baseline 2: TF-IDF + OneVsRest SGD (FAST, paper-usable)\n",
        "# -------------------------\n",
        "pred_test = None\n",
        "gold_test = None\n",
        "idx_test = None\n",
        "\n",
        "if FAST_ML_BASELINE:\n",
        "    X = df_eval[\"narrative\"].values\n",
        "    Ybin = mlb.fit_transform(Y_gold_all)\n",
        "\n",
        "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "        X, Ybin, np.arange(len(df_eval)), test_size=TEST_SIZE, random_state=SEED\n",
        "    )\n",
        "\n",
        "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=80000)\n",
        "    Xtr = vec.fit_transform(X_train)\n",
        "    Xte = vec.transform(X_test)\n",
        "\n",
        "    print(\"\\nTraining TF-IDF + OneVsRest(SGDClassifier) ... (FAST)\")\n",
        "    clf = OneVsRestClassifier(\n",
        "        SGDClassifier(loss=\"log_loss\", alpha=1e-5, max_iter=25, tol=1e-3)\n",
        "    )\n",
        "    clf.fit(Xtr, y_train)\n",
        "\n",
        "    # predict_proba exists for log_loss; returns prob matrix\n",
        "    probs = clf.predict_proba(Xte)\n",
        "    y_pred = (probs >= 0.5).astype(int)\n",
        "\n",
        "    pred_test = [[label_space[j] for j in np.where(row == 1)[0]] for row in y_pred]\n",
        "    gold_test = [Y_gold_all[i] for i in idx_test]\n",
        "\n",
        "    m_tfidf = compute_metrics(gold_test, pred_test)\n",
        "    rows.append({\"method\":\"TF-IDF + OvR SGD (test split)\", **m_tfidf})\n",
        "\n",
        "# -------------------------\n",
        "# 7) Save metrics\n",
        "# -------------------------\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "print(metrics_df)\n",
        "metrics_df.to_csv(\"paper_results_metrics.csv\", index=False)\n",
        "\n",
        "# -------------------------\n",
        "# 8) Figures (use ML baseline test split if available; else use gold_all vs rule)\n",
        "# -------------------------\n",
        "if FAST_ML_BASELINE and (pred_test is not None):\n",
        "    yt = mlb.fit_transform(gold_test)\n",
        "    yp = mlb.transform(pred_test)\n",
        "    title_suffix = \" (test split, TF-IDF)\"\n",
        "else:\n",
        "    yt = mlb.fit_transform(Y_gold_all)\n",
        "    yp = mlb.transform(df_eval[\"pred_rule\"].tolist())\n",
        "    title_suffix = \" (rule baseline)\"\n",
        "\n",
        "support = np.asarray(yt.sum(axis=0)).ravel()\n",
        "tp = np.asarray((yt.multiply(yp)).sum(axis=0)).ravel()\n",
        "rec = np.divide(tp, support, out=np.zeros_like(tp, dtype=float), where=support > 0)\n",
        "\n",
        "top_idx = np.argsort(-support)[:20]\n",
        "top_labels = [label_space[i] for i in top_idx]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), support[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Support (count)\")\n",
        "plt.title(\"Top-20 Label Support\" + title_suffix)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_support.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(range(len(top_idx)), rec[top_idx])\n",
        "plt.xticks(range(len(top_idx)), [l.split(\":\")[0] for l in top_labels], rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Recall on Top-20 Labels\" + title_suffix)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figure_top_labels_recall.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 9) Debug CSV\n",
        "# -------------------------\n",
        "debug = df_eval[[\"incident_id\",\"narrative\",\"gold_topk\"]].copy()\n",
        "debug[\"gold_topk\"] = debug[\"gold_topk\"].apply(lambda x: \"; \".join(x))\n",
        "debug[\"pred_rule\"] = df_eval[\"pred_rule\"].apply(lambda x: \"; \".join(x))\n",
        "\n",
        "# Put ML preds only for test rows\n",
        "pred_ml_full = [\"\"] * len(df_eval)\n",
        "if FAST_ML_BASELINE and (pred_test is not None):\n",
        "    for local_i, global_i in enumerate(idx_test):\n",
        "        pred_ml_full[global_i] = \"; \".join(pred_test[local_i])\n",
        "debug[\"pred_tfidf_testonly\"] = pred_ml_full\n",
        "\n",
        "debug.to_csv(\"paper_predictions_debug.csv\", index=False)\n",
        "\n",
        "# -------------------------\n",
        "# 10) Zip + download\n",
        "# -------------------------\n",
        "artifacts = [\n",
        "    \"paper_results_metrics.csv\",\n",
        "    \"paper_predictions_debug.csv\",\n",
        "    \"figure_top_labels_support.png\",\n",
        "    \"figure_top_labels_recall.png\",\n",
        "]\n",
        "zip_name = \"paper_artifacts.zip\"\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in artifacts:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        print(\" -\", f)\n",
        "\n",
        "print(\"\\nDownloading...\")\n",
        "for f in artifacts + [zip_name]:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)\n",
        "\n",
        "print(\"\\nALL DONE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "P81K1h1jWGZn",
        "outputId": "c07a8871-4068-4a88-818a-66e7e9b544df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENV: numpy 1.26.4 | pandas 2.2.2\n",
            "\n",
            "[UPLOAD] Please upload your files (csv/xlsx):\n",
            " - asrs_paper_dataset_sample.*  (preferred)\n",
            " - asrs_paper_dataset.*         (fallback)\n",
            " - asrs_narratives_clean.*      (optional)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9beddde5-90da-4714-9df5-d2ffd391ec6b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9beddde5-90da-4714-9df5-d2ffd391ec6b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving asrs_narratives_clean.csv to asrs_narratives_clean (1).csv\n",
            "Saving asrs_paper_dataset.csv to asrs_paper_dataset (1).csv\n",
            "Saving asrs_paper_dataset_sample.csv to asrs_paper_dataset_sample (1).csv\n",
            "\n",
            "Uploaded:\n",
            " - asrs_narratives_clean (1).csv\n",
            " - asrs_paper_dataset (1).csv\n",
            " - asrs_paper_dataset_sample (1).csv\n",
            "\n",
            "Using: asrs_paper_dataset_sample (1).csv | shape=(2000, 3)\n",
            "\n",
            "TOPK=50 | eval rows=687/2000\n",
            "Top 10 labels: ['Reference: X', 'Altitude.AGL.Single Value: 0.0', 'Crew Size.Number Of Crew: Flight Crew Size', 'ASRS Report Number.Accession Number: ACN', 'Flight Plan: IFR', 'Detector.Person: Flight Crew', 'Aircraft Operator: Air Carrier', 'Operating Under FAR Part: 121', 'Altitude.MSL.Single Value: MSL', 'Flight Conditions: VMC']\n",
            "\n",
            "Training TF-IDF + OneVsRest(SGDClassifier) ... (FAST)\n",
            "\n",
            "=== PAPER METRICS ===\n",
            "                          method  micro_f1  macro_f1  micro_precision  \\\n",
            "0      Rule baseline (substring)  0.086290  0.073068         0.233141   \n",
            "1  TF-IDF + OvR SGD (test split)  0.499657  0.229148         0.698656   \n",
            "\n",
            "   micro_recall  \n",
            "0      0.052942  \n",
            "1      0.388889  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'multiply'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2324093358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupport\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'multiply'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ASRS multi-label baseline: Rule baseline + TFIDF OvR(SGD)\n",
        "# numpy 1.26.4 | pandas 2.2.2\n",
        "# Fix for yt.multiply(yp): works for dense numpy arrays\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Config\n",
        "# ----------------------------\n",
        "DATA_PATH = \"asrs_paper_dataset_sample (1).csv\"   # <-- 改成你的文件名\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.30\n",
        "TOPK = 50\n",
        "\n",
        "# 若你的标签分隔符不是这些，可以自行加\n",
        "LABEL_SEPS = [r\"\\|\\|\", r\"\\|\", r\";\", r\",\", r\"\\t\"]\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Load CSV\n",
        "# ----------------------------\n",
        "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Loaded:\", DATA_PATH, \"| shape =\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(df.head(3))\n",
        "\n",
        "# ----------------------------\n",
        "# 2) 自动识别文本列 & 标签列\n",
        "#    - 文本列: 平均长度更长、字符串为主\n",
        "#    - 标签列: 往往包含分隔符/冒号/短 token，且唯一值更“离散”\n",
        "# ----------------------------\n",
        "def guess_text_and_label_cols(df: pd.DataFrame):\n",
        "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "    if len(obj_cols) == 0:\n",
        "        raise ValueError(\"No object columns found; cannot guess text/label columns.\")\n",
        "\n",
        "    # 计算每列的平均字符串长度、空值比例\n",
        "    stats = []\n",
        "    for c in obj_cols:\n",
        "        s = df[c].astype(str)\n",
        "        avg_len = s.str.len().mean()\n",
        "        null_rate = df[c].isna().mean()\n",
        "        # 含分隔符的比例（粗略）\n",
        "        sep_hit = s.str.contains(r\"(\\|\\||\\||;|,|\\t)\").mean()\n",
        "        colon_hit = s.str.contains(\":\").mean()\n",
        "        uniq_ratio = df[c].nunique() / max(len(df), 1)\n",
        "        stats.append((c, avg_len, null_rate, sep_hit, colon_hit, uniq_ratio))\n",
        "\n",
        "    # 文本列: avg_len 大，sep_hit 相对小\n",
        "    # 标签列: sep_hit/colon_hit 高 或 avg_len 相对小\n",
        "    # 这里给个启发式打分\n",
        "    def text_score(x):\n",
        "        c, avg_len, null_rate, sep_hit, colon_hit, uniq_ratio = x\n",
        "        return avg_len - 50*sep_hit - 20*colon_hit - 10*null_rate\n",
        "\n",
        "    def label_score(x):\n",
        "        c, avg_len, null_rate, sep_hit, colon_hit, uniq_ratio = x\n",
        "        return 80*sep_hit + 30*colon_hit - 0.2*avg_len - 10*null_rate\n",
        "\n",
        "    text_col = max(stats, key=text_score)[0]\n",
        "    label_col = max([x for x in stats if x[0] != text_col], key=label_score, default=None)\n",
        "    if label_col is None:\n",
        "        # 如果只有一个 object 列，那就没法猜；直接报错\n",
        "        raise ValueError(\"Only one object column found; cannot determine label column.\")\n",
        "    label_col = label_col[0]\n",
        "    return text_col, label_col, stats\n",
        "\n",
        "text_col, label_col, col_stats = guess_text_and_label_cols(df)\n",
        "\n",
        "print(\"\\n[Auto-detected]\")\n",
        "print(\"  text_col :\", text_col)\n",
        "print(\"  label_col:\", label_col)\n",
        "\n",
        "print(\"\\n[Column stats: col, avg_len, null_rate, sep_hit, colon_hit, uniq_ratio]\")\n",
        "for row in col_stats:\n",
        "    print(\" \", row)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) 解析多标签\n",
        "#    输入标签可能是：\n",
        "#      - \"A || B || C\"\n",
        "#      - \"A;B;C\"\n",
        "#      - \"['A','B']\"\n",
        "#      - 单标签字符串\n",
        "# ----------------------------\n",
        "_sep_regex = re.compile(\"|\".join(LABEL_SEPS))\n",
        "\n",
        "def parse_labels(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    # 若是类似 \"['a', 'b']\" 这种\n",
        "    s = str(x).strip()\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        # 轻量安全解析：去掉括号、引号，再按逗号切\n",
        "        inner = s[1:-1].strip()\n",
        "        if not inner:\n",
        "            return []\n",
        "        parts = [p.strip().strip(\"'\").strip('\"') for p in inner.split(\",\")]\n",
        "        return [p for p in parts if p]\n",
        "    # 常规：按分隔符切\n",
        "    parts = [p.strip() for p in _sep_regex.split(s)]\n",
        "    parts = [p for p in parts if p]\n",
        "    return parts\n",
        "\n",
        "texts = df[text_col].astype(str).fillna(\"\").tolist()\n",
        "y_list = df[label_col].apply(parse_labels).tolist()\n",
        "\n",
        "# 去掉完全没标签的样本（可选）\n",
        "keep_idx = [i for i, lab in enumerate(y_list) if len(lab) > 0]\n",
        "texts = [texts[i] for i in keep_idx]\n",
        "y_list = [y_list[i] for i in keep_idx]\n",
        "\n",
        "print(f\"\\nAfter removing empty-label rows: n={len(texts)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4) MultiLabelBinarizer\n",
        "# ----------------------------\n",
        "mlb = MultiLabelBinarizer(sparse_output=False)  # 这里输出 dense，后面就别用 .multiply 了\n",
        "Y = mlb.fit_transform(y_list)\n",
        "labels = mlb.classes_\n",
        "\n",
        "print(\"Num labels:\", len(labels))\n",
        "# 统计 Top labels\n",
        "support = Y.sum(axis=0)\n",
        "top_idx = np.argsort(-support)[:10]\n",
        "print(\"Top 10 labels:\", [labels[i] for i in top_idx])\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Train/Test split\n",
        "# ----------------------------\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    texts, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit: train={len(X_train)} test={len(X_test)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Metrics utilities\n",
        "# ----------------------------\n",
        "def multilabel_micro_macro(Y_true, Y_pred):\n",
        "    \"\"\"\n",
        "    Y_true, Y_pred: numpy arrays (n_samples, n_labels), binary 0/1\n",
        "    returns dict with micro/macro precision/recall/f1\n",
        "    \"\"\"\n",
        "    Y_true = (Y_true > 0).astype(int)\n",
        "    Y_pred = (Y_pred > 0).astype(int)\n",
        "\n",
        "    # --- per-label stats ---\n",
        "    support = Y_true.sum(axis=0).astype(float)                      # positives per label\n",
        "    pred_pos = Y_pred.sum(axis=0).astype(float)                     # predicted positives per label\n",
        "    tp = (Y_true * Y_pred).sum(axis=0).astype(float)                # ✅ dense elementwise multiply (fix)\n",
        "    # per-label precision/recall\n",
        "    prec = np.divide(tp, pred_pos, out=np.zeros_like(tp), where=pred_pos > 0)\n",
        "    rec  = np.divide(tp, support,  out=np.zeros_like(tp), where=support > 0)\n",
        "    f1   = np.divide(2*prec*rec, (prec+rec), out=np.zeros_like(tp), where=(prec+rec) > 0)\n",
        "\n",
        "    # macro averages over labels that appear in true set (support>0)\n",
        "    mask = support > 0\n",
        "    macro_p = prec[mask].mean() if mask.any() else 0.0\n",
        "    macro_r = rec[mask].mean()  if mask.any() else 0.0\n",
        "    macro_f1 = f1[mask].mean()  if mask.any() else 0.0\n",
        "\n",
        "    # micro\n",
        "    TP = tp.sum()\n",
        "    P  = pred_pos.sum()\n",
        "    T  = support.sum()\n",
        "    micro_p = TP / P if P > 0 else 0.0\n",
        "    micro_r = TP / T if T > 0 else 0.0\n",
        "    micro_f1 = (2*micro_p*micro_r/(micro_p+micro_r)) if (micro_p+micro_r)>0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"micro_precision\": micro_p,\n",
        "        \"micro_recall\": micro_r,\n",
        "        \"micro_f1\": micro_f1,\n",
        "        \"macro_precision\": macro_p,\n",
        "        \"macro_recall\": macro_r,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"per_label\": {\n",
        "            \"support\": support,\n",
        "            \"pred_pos\": pred_pos,\n",
        "            \"tp\": tp,\n",
        "            \"precision\": prec,\n",
        "            \"recall\": rec,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Rule baseline (substring)\n",
        "#    用训练集里 topK labels 做简单 substring 匹配\n",
        "# ----------------------------\n",
        "# 选 topK labels（按训练集支持度）\n",
        "train_support = Y_train.sum(axis=0)\n",
        "topk_idx = np.argsort(-train_support)[:TOPK]\n",
        "topk_labels = [labels[i] for i in topk_idx]\n",
        "\n",
        "def rule_predict(texts, topk_labels):\n",
        "    preds = np.zeros((len(texts), len(labels)), dtype=int)\n",
        "    # 只在 topK 上预测，其他全 0\n",
        "    label_to_i = {lab: i for i, lab in enumerate(labels)}\n",
        "    for r, t in enumerate(texts):\n",
        "        t_low = str(t).lower()\n",
        "        for lab in topk_labels:\n",
        "            # 简单 substring：label 字符串出现在 text 里就预测为 1\n",
        "            if lab.lower() in t_low:\n",
        "                preds[r, label_to_i[lab]] = 1\n",
        "    return preds\n",
        "\n",
        "Y_pred_rule = rule_predict(X_test, topk_labels)\n",
        "m_rule = multilabel_micro_macro(Y_test, Y_pred_rule)\n",
        "\n",
        "# ----------------------------\n",
        "# 8) TF-IDF + OneVsRest(SGDClassifier)\n",
        "# ----------------------------\n",
        "print(\"\\nTraining TF-IDF + OneVsRest(SGDClassifier) ... (FAST)\")\n",
        "\n",
        "vec = TfidfVectorizer(\n",
        "    max_features=200000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.98,\n",
        "    strip_accents=\"unicode\"\n",
        ")\n",
        "\n",
        "Xtr = vec.fit_transform(X_train)\n",
        "Xte = vec.transform(X_test)\n",
        "\n",
        "clf = OneVsRestClassifier(\n",
        "    SGDClassifier(\n",
        "        loss=\"log_loss\",      # 逻辑回归风格，可输出概率/decision\n",
        "        alpha=1e-5,\n",
        "        penalty=\"l2\",\n",
        "        max_iter=2000,\n",
        "        tol=1e-3,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "clf.fit(Xtr, Y_train)\n",
        "\n",
        "# 预测：这里给一个简单阈值 0（decision_function），也可改成 topk per row\n",
        "if hasattr(clf, \"decision_function\"):\n",
        "    scores = clf.decision_function(Xte)\n",
        "    Y_pred_sgd = (scores > 0).astype(int)\n",
        "else:\n",
        "    Y_pred_sgd = clf.predict(Xte).astype(int)\n",
        "\n",
        "m_sgd = multilabel_micro_macro(Y_test, Y_pred_sgd)\n",
        "\n",
        "# ----------------------------\n",
        "# 9) Print summary table (like your screenshot)\n",
        "# ----------------------------\n",
        "res = pd.DataFrame([\n",
        "    {\n",
        "        \"method\": \"Rule baseline (substring)\",\n",
        "        \"micro_f1\": m_rule[\"micro_f1\"],\n",
        "        \"macro_f1\": m_rule[\"macro_f1\"],\n",
        "        \"micro_precision\": m_rule[\"micro_precision\"],\n",
        "        \"micro_recall\": m_rule[\"micro_recall\"],\n",
        "    },\n",
        "    {\n",
        "        \"method\": \"TF-IDF + OvR SGD (test split)\",\n",
        "        \"micro_f1\": m_sgd[\"micro_f1\"],\n",
        "        \"macro_f1\": m_sgd[\"macro_f1\"],\n",
        "        \"micro_precision\": m_sgd[\"micro_precision\"],\n",
        "        \"micro_recall\": m_sgd[\"micro_recall\"],\n",
        "    }\n",
        "])\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "print(res)\n",
        "\n",
        "# ----------------------------\n",
        "# 10) 额外：top labels 的 per-label recall（便于 debug）\n",
        "# ----------------------------\n",
        "per = m_sgd[\"per_label\"]\n",
        "support = per[\"support\"]\n",
        "recall = per[\"recall\"]\n",
        "f1 = per[\"f1\"]\n",
        "\n",
        "top_lbl = np.argsort(-support)[:20]\n",
        "dbg = pd.DataFrame({\n",
        "    \"label\": [labels[i] for i in top_lbl],\n",
        "    \"support\": support[top_lbl].astype(int),\n",
        "    \"recall\": recall[top_lbl],\n",
        "    \"f1\": f1[top_lbl],\n",
        "})\n",
        "print(\"\\n=== Top-20 labels (by support) — per-label recall/f1 (SGD) ===\")\n",
        "print(dbg.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "yB1aQL9GSH6g",
        "outputId": "cbc46d45-82dc-41bd-9fc4-7dc4a22d22f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "File not found: asrs_paper_dataset_sample (1).csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4225710032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 1) Load CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"File not found: {DATA_PATH}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: File not found: asrs_paper_dataset_sample (1).csv"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ASRS multi-label baseline with FILE UPLOAD (Colab-ready)\n",
        "# numpy 1.26.4 | pandas 2.2.2\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Environment + Upload\n",
        "# ----------------------------\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"numpy\", np.__version__, \"| pandas\", pd.__version__)\n",
        "print(\"\\n[UPLOAD] Please upload your files (csv/xlsx):\")\n",
        "print(\" - asrs_paper_dataset_sample.*  (preferred)\")\n",
        "print(\" - asrs_paper_dataset.*         (fallback)\")\n",
        "print(\" - asrs_narratives_clean.*      (optional)\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"\\nUploaded:\")\n",
        "for k in uploaded:\n",
        "    print(\" -\", k)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Choose dataset (priority)\n",
        "# ----------------------------\n",
        "def pick_dataset(files_dict):\n",
        "    names = list(files_dict.keys())\n",
        "    # priority order\n",
        "    for key in names:\n",
        "        if \"asrs_paper_dataset_sample\" in key:\n",
        "            return key\n",
        "    for key in names:\n",
        "        if \"asrs_paper_dataset\" in key:\n",
        "            return key\n",
        "    raise ValueError(\"No ASRS dataset found in uploaded files.\")\n",
        "\n",
        "DATA_FILE = pick_dataset(uploaded)\n",
        "print(f\"\\nUsing: {DATA_FILE}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Load CSV / XLSX\n",
        "# ----------------------------\n",
        "if DATA_FILE.endswith(\".csv\"):\n",
        "    df = pd.read_csv(DATA_FILE)\n",
        "elif DATA_FILE.endswith(\".xlsx\"):\n",
        "    df = pd.read_excel(DATA_FILE)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported file type.\")\n",
        "\n",
        "print(\"shape =\", df.shape)\n",
        "print(\"columns =\", list(df.columns))\n",
        "display(df.head())\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Guess text column & label column\n",
        "# ----------------------------\n",
        "def guess_text_and_label_cols(df):\n",
        "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "    if len(obj_cols) < 2:\n",
        "        raise ValueError(\"Need at least 2 string columns.\")\n",
        "\n",
        "    stats = []\n",
        "    for c in obj_cols:\n",
        "        s = df[c].astype(str)\n",
        "        stats.append({\n",
        "            \"col\": c,\n",
        "            \"avg_len\": s.str.len().mean(),\n",
        "            \"sep\": s.str.contains(r\"\\|\\||\\||;|,|\\t\").mean(),\n",
        "            \"colon\": s.str.contains(\":\").mean()\n",
        "        })\n",
        "\n",
        "    text_col = max(stats, key=lambda x: x[\"avg_len\"])[\"col\"]\n",
        "    label_col = max(\n",
        "        [x for x in stats if x[\"col\"] != text_col],\n",
        "        key=lambda x: x[\"sep\"] + x[\"colon\"]\n",
        "    )[\"col\"]\n",
        "\n",
        "    return text_col, label_col, stats\n",
        "\n",
        "text_col, label_col, stats = guess_text_and_label_cols(df)\n",
        "\n",
        "print(\"\\n[Auto-detected columns]\")\n",
        "print(\" text_col :\", text_col)\n",
        "print(\" label_col:\", label_col)\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Parse multi-labels\n",
        "# ----------------------------\n",
        "LABEL_SEPS = re.compile(r\"\\|\\||\\||;|,|\\t\")\n",
        "\n",
        "def parse_labels(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    s = str(x).strip()\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        s = s[1:-1]\n",
        "    return [p.strip().strip(\"'\").strip('\"')\n",
        "            for p in LABEL_SEPS.split(s) if p.strip()]\n",
        "\n",
        "texts = df[text_col].astype(str).fillna(\"\").tolist()\n",
        "y_list = df[label_col].apply(parse_labels).tolist()\n",
        "\n",
        "# drop empty-label rows\n",
        "keep = [i for i, y in enumerate(y_list) if len(y) > 0]\n",
        "texts = [texts[i] for i in keep]\n",
        "y_list = [y_list[i] for i in keep]\n",
        "\n",
        "print(f\"\\nEffective rows: {len(texts)}/{len(df)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5) MultiLabelBinarizer\n",
        "# ----------------------------\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer(sparse_output=False)\n",
        "Y = mlb.fit_transform(y_list)\n",
        "labels = mlb.classes_\n",
        "\n",
        "support = Y.sum(axis=0)\n",
        "top10 = np.argsort(-support)[:10]\n",
        "print(\"\\nTop 10 labels:\", [labels[i] for i in top10])\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Train / Test split\n",
        "# ----------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtr_txt, Xte_txt, Ytr, Yte = train_test_split(\n",
        "    texts, Y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Split: train={len(Xtr_txt)} test={len(Xte_txt)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Metrics (NUMPY SAFE)\n",
        "# ----------------------------\n",
        "def multilabel_metrics(Y_true, Y_pred):\n",
        "    Y_true = (Y_true > 0).astype(int)\n",
        "    Y_pred = (Y_pred > 0).astype(int)\n",
        "\n",
        "    tp = (Y_true * Y_pred).sum(axis=0)\n",
        "    support = Y_true.sum(axis=0)\n",
        "    pred_pos = Y_pred.sum(axis=0)\n",
        "\n",
        "    prec = np.divide(tp, pred_pos, out=np.zeros_like(tp, float), where=pred_pos > 0)\n",
        "    rec  = np.divide(tp, support,  out=np.zeros_like(tp, float), where=support > 0)\n",
        "    f1   = np.divide(2*prec*rec, prec+rec, out=np.zeros_like(tp, float), where=(prec+rec)>0)\n",
        "\n",
        "    mask = support > 0\n",
        "    macro_f1 = f1[mask].mean() if mask.any() else 0.0\n",
        "\n",
        "    TP, P, T = tp.sum(), pred_pos.sum(), support.sum()\n",
        "    micro_p = TP/P if P>0 else 0\n",
        "    micro_r = TP/T if T>0 else 0\n",
        "    micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r) if (micro_p+micro_r)>0 else 0\n",
        "\n",
        "    return micro_f1, macro_f1, micro_p, micro_r\n",
        "\n",
        "# ----------------------------\n",
        "# 8) Rule baseline (substring)\n",
        "# ----------------------------\n",
        "TOPK = 50\n",
        "topk_idx = np.argsort(-Ytr.sum(axis=0))[:TOPK]\n",
        "topk_labels = [labels[i] for i in topk_idx]\n",
        "label_to_i = {l:i for i,l in enumerate(labels)}\n",
        "\n",
        "Y_pred_rule = np.zeros_like(Yte)\n",
        "for r, t in enumerate(Xte_txt):\n",
        "    t = t.lower()\n",
        "    for lab in topk_labels:\n",
        "        if lab.lower() in t:\n",
        "            Y_pred_rule[r, label_to_i[lab]] = 1\n",
        "\n",
        "m_rule = multilabel_metrics(Yte, Y_pred_rule)\n",
        "\n",
        "# ----------------------------\n",
        "# 9) TF-IDF + OvR(SGD)\n",
        "# ----------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "print(\"\\nTraining TF-IDF + OneVsRest(SGDClassifier) ... (FAST)\")\n",
        "\n",
        "vec = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2,\n",
        "    max_df=0.98,\n",
        "    max_features=200_000\n",
        ")\n",
        "\n",
        "Xtr = vec.fit_transform(Xtr_txt)\n",
        "Xte = vec.transform(Xte_txt)\n",
        "\n",
        "clf = OneVsRestClassifier(\n",
        "    SGDClassifier(\n",
        "        loss=\"log_loss\",\n",
        "        alpha=1e-5,\n",
        "        max_iter=2000,\n",
        "        tol=1e-3,\n",
        "        random_state=42\n",
        "    ),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "clf.fit(Xtr, Ytr)\n",
        "\n",
        "scores = clf.decision_function(Xte)\n",
        "Y_pred_sgd = (scores > 0).astype(int)\n",
        "\n",
        "m_sgd = multilabel_metrics(Yte, Y_pred_sgd)\n",
        "\n",
        "# ----------------------------\n",
        "# 10) Paper-style summary\n",
        "# ----------------------------\n",
        "res = pd.DataFrame([\n",
        "    {\n",
        "        \"method\": \"Rule baseline (substring)\",\n",
        "        \"micro_f1\": m_rule[0],\n",
        "        \"macro_f1\": m_rule[1],\n",
        "        \"micro_precision\": m_rule[2],\n",
        "        \"micro_recall\": m_rule[3],\n",
        "    },\n",
        "    {\n",
        "        \"method\": \"TF-IDF + OvR SGD (test split)\",\n",
        "        \"micro_f1\": m_sgd[0],\n",
        "        \"macro_f1\": m_sgd[1],\n",
        "        \"micro_precision\": m_sgd[2],\n",
        "        \"micro_recall\": m_sgd[3],\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n=== PAPER METRICS ===\")\n",
        "display(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "5sItGmE9Sem1",
        "outputId": "95a33dad-fc76-4abc-dfa3-7ed01d73fc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy 2.0.2 | pandas 2.2.2\n",
            "\n",
            "[UPLOAD] Please upload your files (csv/xlsx):\n",
            " - asrs_paper_dataset_sample.*  (preferred)\n",
            " - asrs_paper_dataset.*         (fallback)\n",
            " - asrs_narratives_clean.*      (optional)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11940ae9-ff37-444e-884b-f64f0db91ab3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-11940ae9-ff37-444e-884b-f64f0db91ab3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving asrs_narratives_clean.csv to asrs_narratives_clean.csv\n",
            "Saving asrs_paper_dataset.csv to asrs_paper_dataset.csv\n",
            "Saving asrs_paper_dataset_sample.csv to asrs_paper_dataset_sample.csv\n",
            "\n",
            "Uploaded:\n",
            " - asrs_narratives_clean.csv\n",
            " - asrs_paper_dataset.csv\n",
            " - asrs_paper_dataset_sample.csv\n",
            "\n",
            "Using: asrs_paper_dataset_sample.csv\n",
            "shape = (2000, 3)\n",
            "columns = ['incident_id', 'narrative', 'labels']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   incident_id                                          narrative  \\\n",
              "0       557782  DOWNWIND FROM DIETZ TO DFW RWY 17C APCH, DECLA...   \n",
              "1       134261  AT FL190 35 NM FROM BUF LOUD BUZZING WAS HEARD...   \n",
              "2       535549  MULTIPLE ACR'S DEPARTING RWY 7 AT LAS. ON THIS...   \n",
              "3       216923  AFTER A PRESSURIZATION PROBLEM AND DIVERSION D...   \n",
              "4       804965  WHILE PERFORMING R AND L ENG COMPRESSOR WASHES...   \n",
              "\n",
              "                                              labels  \n",
              "0  ASRS Report Number.Accession Number: nan; ATC ...  \n",
              "1  ASRS Report Number.Accession Number: nan; Airc...  \n",
              "2  ASRS Report Number.Accession Number: nan; Alti...  \n",
              "3  ASRS Report Number.Accession Number: nan; Airc...  \n",
              "4  ASRS Report Number.Accession Number: nan; Airc...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e872185-e0d3-41ef-973e-0ddad47972b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>narrative</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>557782</td>\n",
              "      <td>DOWNWIND FROM DIETZ TO DFW RWY 17C APCH, DECLA...</td>\n",
              "      <td>ASRS Report Number.Accession Number: nan; ATC ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>134261</td>\n",
              "      <td>AT FL190 35 NM FROM BUF LOUD BUZZING WAS HEARD...</td>\n",
              "      <td>ASRS Report Number.Accession Number: nan; Airc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>535549</td>\n",
              "      <td>MULTIPLE ACR'S DEPARTING RWY 7 AT LAS. ON THIS...</td>\n",
              "      <td>ASRS Report Number.Accession Number: nan; Alti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>216923</td>\n",
              "      <td>AFTER A PRESSURIZATION PROBLEM AND DIVERSION D...</td>\n",
              "      <td>ASRS Report Number.Accession Number: nan; Airc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>804965</td>\n",
              "      <td>WHILE PERFORMING R AND L ENG COMPRESSOR WASHES...</td>\n",
              "      <td>ASRS Report Number.Accession Number: nan; Airc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e872185-e0d3-41ef-973e-0ddad47972b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e872185-e0d3-41ef-973e-0ddad47972b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e872185-e0d3-41ef-973e-0ddad47972b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(res)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 273382,\n        \"min\": 134261,\n        \"max\": 804965,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          134261,\n          804965,\n          535549\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"narrative\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"AT FL190 35 NM FROM BUF LOUD BUZZING WAS HEARD. ELECTRICAL SYSTEM SHUTDOWN AND SMOKE FILLED THE COCKPIT. ELECTRICAL SYSTEM CAME BACK ON LINE WITH LEFT GENERATOR OFF LIGHT ILLUMINATED. CHECKLIST WAS ACCOMPLISHED, SMOKE SUBSIDED, AND AN EMER WAS DECLARED WITH A REQUESTED RETURN TO BUF. BUF WX WAS APPROX 25 OVC 10 USING RWY 23. AN EXPEDITIOUS DSCNT AND ILS RWY 23 WAS ACCOMPLISHED. EQUIPMENT WAS REQUESTED TO STAND BY. AN UNEVENTFUL LNDG WAS MADE. SMOKE INCREASED SLIGHTLY AS ACFT APCHED THE GATE. SO COCKPIT WINDOWS WERE OPENED, ACFT POWER WAS SHUT DOWN AT THE GATE WITH EXIT LIGHTS ON. PAX WERE DIRECTED TO LEAVE ACFT WITHOUT BAGGAGE. EQUIPMENT WAS REQUESTED TO APCH THE ACFT AND CHECK FOR ANY REMAINING FIRE.\",\n          \"WHILE PERFORMING R AND L ENG COMPRESSOR WASHES, LEFT ONE OF THE SHORT HOSES THAT CONNECT THE SPRAY NOZZLE RAILS TOGETHER. HOSE WAS LEFT ON THE TOP R UPPER ENG COWL. TO THE BEST OF MY KNOWLEDGE, ALL OF THE EQUIP FOR THE COMPRESSOR WASH KIT WAS IN THE BOX. THE RAILS AND HOSES WERE PUT AWAY AS SOON AS IT WAS REMOVED FROM THE ENG. IF THE STORAGE BOX FOR THE EQUIP WAS SHADOWED MIGHT HELP IN IDENTIFYING ANY MISSING PARTS.\",\n          \"MULTIPLE ACR'S DEPARTING RWY 7 AT LAS. ON THIS CURRENT RWY CONFIGN LNDG DEPARTING RWY 1 (PRIMARY ARR RWY) DEPARTING RWY 7 (PRIMARY DEP RWY). ACFT DEPARTING RWY 7 ON THE MINEY 1 AND MCCARREN 1 DEP FLY OVER BOULDER CITY VOR (BLD) CLBING TO 7000 FT INITIALLY. THEY DEPART IN E DEP'S AIRSPACE, THEY TALK TO W DEP. THEY ENTER W DEP'S AIRSPACE AFTER ABOUT 20 FLYING MILES. SO THEY STAY IN E DEP'S AIRSPACE UNTIL THEY ALMOST REACH THE VERT LIMITS OF E DEP'S AIRSPACE WITH ZLA. THERE IS KNOWN SKYDIVING OPS DAILY AT BOULDER CITY ARPT (61B). BLD 162 DEG RADIAL, 2.88 DME, WHICH IS IN THE FLT PATH OF THE ABOVE MENTIONED DEPS. THE SKYDIVE ACFT REQUEST 10500 FT THROUGH 12500 FT FOR THEIR DROP. BEING THAT SKYDIVE IS ON E DEP'S FREQ AND THESE DEPS ARE ON W DEP'S FREQ, AGAIN THESE DEPS ARE IN E DEP'S AIRSPACE. MANY OF US (THE CTLRS) BELIEVE THIS IS TOTALLY UNSAFE. DEPS IN ONE CTLR'S AIRSPACE TALKING TO ANOTHER. WHEN THERE ARE OVERFLTS IN THIS AREA THEY TALK TO E DEP. THIS CREATES A COORD NIGHTMARE. MANY CTLRS WORKING THE FLOOR, FEEL THAT IS ONLY A MATTER OF TIME BEFORE THERE IS AN OPERROR. CALLBACK CONVERSATION WITH RPTR REVEALED THE FOLLOWING INFO: RPTR ADVISED THAT THE AIRSPACE REORGANIZATION IS AGAIN UNDER REVIEW. THERE IS LOTS OF FINGER POINTING THAT HAS BECOME COUNTERPRODUCTIVE FOR BOTH CTLR AND SUPVR. A NATIONAL AIRSPACE WORK GROUP HAS ALSO BECOME INVOLVED. SUPPLEMENTAL INFO: A SPECIALIST ADVISED THAT THE AIRSPACE IMPLEMENTATION WAS NOT WELL HANDLED. EFFORTS ARE UNDERWAY TO RESOLVE ISSUES AND CONCERNS.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ASRS Report Number.Accession Number: nan; Aircraft Operator: nan; Altitude.MSL.Single Value: 19000; Anomaly.Other: nan; Crew Size.Number Of Crew: nan; Detector.Person: nan; Experience.Flight Crew.Type: nan; Flight Phase: nan; Flight Plan: nan; Function.Air Traffic Control: nan; Function.Flight Crew: nan; Light: nan; Local Time Of Day: nan; Make Model Name: nan; Primary Problem: nan; Qualification.Air Traffic Control: nan; Reference: nan; Reporter Organization: nan; Result.Flight Crew: nan; State Reference: nan\",\n          \"ASRS Report Number.Accession Number: nan; Aircraft Operator: nan; Altitude.AGL.Single Value: 0; Anomaly.Deviation - Procedural: nan; Batch Number: nan; Contributing Factors / Situations: nan; Date: nan; Detector.Person: nan; Flight Phase: nan; Human Factors: nan; Local Time Of Day: nan; Locale Reference.Airport: nan; Make Model Name: nan; Mission: nan; Operating Under FAR Part: nan; Primary Problem: nan; Reference: nan; Reporter Organization: nan; Result.General: nan; State Reference: nan\",\n          \"ASRS Report Number.Accession Number: nan; Altitude.MSL.Single Value: 7500.0; Anomaly.Conflict: nan; Anomaly.Other: nan; Batch Number: nan; Contributing Factors / Situations: nan; Date: nan; Detector.Person: nan; Flight Conditions: nan; Function.Air Traffic Control: nan; Light: nan; Local Time Of Day: nan; Locale Reference.Airport: nan; Location Of Person.Facility: nan; Qualification.Air Traffic Control: nan; Reference: nan; Reporter Organization: nan; Result.General: nan; State Reference: nan; Weather Elements / Visibility.Visibility: nan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Auto-detected columns]\n",
            " text_col : narrative\n",
            " label_col: labels\n",
            "\n",
            "Effective rows: 2000/2000\n",
            "\n",
            "Top 10 labels: ['Reference: nan', 'Make Model Name: nan', 'Function.Flight Crew: nan', 'Reporter Organization: nan', 'Date: nan', 'Qualification.Flight Crew: nan', 'ASRS Report Number.Accession Number: nan', 'Flight Phase: nan', 'State Reference: nan', 'Flight Plan: nan']\n",
            "Split: train=1400 test=600\n",
            "\n",
            "Training TF-IDF + OneVsRest(SGDClassifier) ... (FAST)\n",
            "\n",
            "=== PAPER METRICS ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                          method  micro_f1  macro_f1  micro_precision  \\\n",
              "0      Rule baseline (substring)  0.000000  0.000000          0.00000   \n",
              "1  TF-IDF + OvR SGD (test split)  0.570042  0.083796          0.64648   \n",
              "\n",
              "   micro_recall  \n",
              "0      0.000000  \n",
              "1      0.509768  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17674c5d-67b1-4936-83c3-99746cb68e0a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>micro_f1</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>micro_precision</th>\n",
              "      <th>micro_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rule baseline (substring)</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TF-IDF + OvR SGD (test split)</td>\n",
              "      <td>0.570042</td>\n",
              "      <td>0.083796</td>\n",
              "      <td>0.64648</td>\n",
              "      <td>0.509768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17674c5d-67b1-4936-83c3-99746cb68e0a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-17674c5d-67b1-4936-83c3-99746cb68e0a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-17674c5d-67b1-4936-83c3-99746cb68e0a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_9f7d3455-c2e0-4d36-8ab8-1a8762604bd7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('res')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9f7d3455-c2e0-4d36-8ab8-1a8762604bd7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('res');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "res",
              "summary": "{\n  \"name\": \"res\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TF-IDF + OvR SGD (test split)\",\n          \"Rule baseline (substring)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"micro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4030802414157737,\n        \"min\": 0.0,\n        \"max\": 0.5700415441348085,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5700415441348085,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05925239111108146,\n        \"min\": 0.0,\n        \"max\": 0.08379553511232642,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.08379553511232642,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"micro_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.457130122384866,\n        \"min\": 0.0,\n        \"max\": 0.6464796188459503,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.6464796188459503,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"micro_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3604603444585957,\n        \"min\": 0.0,\n        \"max\": 0.5097679078310235,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5097679078310235,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}